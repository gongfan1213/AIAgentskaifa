### 第10章 PEFT微调实战——打造医疗领域LLM
#### （2）根据以下步骤导入项目。
首先，打开PyCharm，选择“File”→“Open”命令，导入项目，如图10 - 30所示。

![image](https://github.com/user-attachments/assets/7de0b4de-4dae-4cae-8961-6be93dcd2106)


其次，选择本书提供的案例项目，如图10 - 31所示。


![image](https://github.com/user-attachments/assets/7c51f948-30b6-4c5c-ab52-d213c3aa41d7)


#### （3）按照以下步骤设置项目开发运行环境。
选择“File”→“Settings”命令，如图10 - 32所示。

![image](https://github.com/user-attachments/assets/31bff4ec-8d6f-4e6d-b5c6-8cb6150ca19e)


选择“Settings”界面左侧“Project:Llama - Agent - Chinese - m”节点中的“Python Interpreter”选项，如图10 - 33所示。

![image](https://github.com/user-attachments/assets/6b4bd67e-7dc5-4e93-a2a7-e135bc4ad6ac)


单击“Settings”界面右上角的“齿轮”图标，在弹出的下拉列表中选择“Add”选项，如图10 - 34所示，进入“Add Python Interpreter”界面。

![image](https://github.com/user-attachments/assets/5f8d88f7-7021-4297-b800-a2ae8daa8fd4)


在“Add Python Interpreter”界面中选中“Existing environment”单选按钮，并单击“...”图标，如图10 - 35所示。


![image](https://github.com/user-attachments/assets/576582e3-a67b-4183-9e6d-fb4f64668546)


在弹出的“Select Python Interpreter”界面中选择已经安装完成的PyTorch环境中的“python.exe”文件，并单击“OK”按钮，如图10 - 36所示。


![image](https://github.com/user-attachments/assets/99f11de5-9ab0-4942-9aba-41bba87d7146)


至此，微调环境已全部搭建完成。

### 10.3 模型微调实战
#### 10.3.1 模型微调整体流程
模型微调实战分为微调和推理两部分，微调流程如图10 - 37所示，推理流程如图10 - 38所示。


![image](https://github.com/user-attachments/assets/4d528a38-42d7-4768-937d-2218ab5957fb)


![image](https://github.com/user-attachments/assets/88f73230-edf7-4a56-8faa-25f0b4bb3c07)


### 10.3.2 项目目录结构说明
Agent - Med - Chinese - main工程目录：
- **data文件夹**：
    - agent_data.json：这个文件包含了用于微调的文本数据。
    - Infer.json：这个文件包含了在推理阶段使用的文本数据。
- **lora - agent - med - finetune文件夹**：用于存储微调过程中生成的模型权重文件。在微调结束后，模型会被保存在这个文件夹中，以便后续的推理或进一步的微调使用。
- **templates文件夹**：
    - med_template.json：这个文件包含了用于生成提示词的模板。在微调过程中，会使用这个模板来辅助生成文本输出。
- **finetune.py**：这是用于执行模型微调（fine - tuning）的代码文件。微调过程通常包括加载预训练模型、加载微调数据、设置及加载微调参数、执行微调循环、保存微调模型等。
- **infer.py**：这是用于执行推理的代码文件。这个文件包含加载已经微调好的模型、准备推理数据、执行推理过程等功能。

整体目录结构如图10 - 39所示。


![image](https://github.com/user-attachments/assets/ec8742ee-e8e7-4636-9eab-9423a3f1dfa4)


#### 10.3.3 基础模型选择
选择合适的预训练模型对于LLM下游任务的成功执行至关重要。市面上有许多主流的LLM可供选择，其中包括Llama、Mistral、Qwen和Yi等，读者可根据自己的任务需要选择合适的模型。

以下是对上面提及的4个基础模型的简单介绍。

1. **Llama**
    - **发布者**：Meta。
    - **特点**：Llama 3是该系列最新的版本，使用超过15万亿个token的预训练。目前，Llama 3已推出80亿（8B）和700亿（70B）参数两个版本，支持8K的上下文窗口。在多个行业基准测试中，Llama 3展现出了领先的性能。
    - **适用场景**：适用于对话系统和聊天任务，尤其是在需要处理大量人类对话样本时表现优秀。

2. **Mistral**
    - **发布者**：Mistral。
    - **性能表现**：Mistral 7B在各项基准测试中表现优秀，超过了Llama 2 13B，在许多基准测试中超过了Llama 1 34B，在代码任务上接近CodeLlama 7B的性能。
    - **适用场景**：适用于文本生成、指导式遵循和代码生成等任务，具有优秀的性能和灵活性。

3. **Qwen**
    - **发布者**：阿里云。
    - **特点**：Qwen是阿里云推出的基于Transformer架构的LLM，使用大量的网络文本、书籍和代码等数据进行预训练。
    - **适用场景**：适用于各种自然语言处理任务，包括文本分类、命名实体识别、问答系统等。

4. **Yi**
    - **特点**：Yi是基于高质量语料库训练的LLM，支持英语和汉语两种语言，其语料库中包含3万亿个令牌。
    - **适用场景**：适用于涉及英语和汉语文本的各种自然语言处理任务，具有较强的语言理解和生成能力。

#### 10.3.4 微调数据集构建
在构建微调数据集时，我们需要准备微调数据、提示词模板与推理数据。

微调数据和推理数据的格式都是JSON，具体包含以下字段。
- **instruction**：指令，问题描述。
- **input**：输入，描述了上下文语境的相关信息。
- **output**：输出，描述了期望得到的结果。

例如：
```json
{
    "instruction": "麻风病和儿童哮喘的病因是否一致？",
    "input": "患者年龄为10岁",
    "output": "麻风病是由麻风分枝杆菌引起的一种慢性接触性传染病，儿童哮喘是一种慢性呼吸道疾病。"
}
```

提示词模板用于生成模型的输入，也采用了JSON格式，包含以下字段。
- **description**：描述了模板的用途。
- **prompt_input**：包含了模板的输入部分，可以根据指令生成问题描述。
- **prompt_no_input**：类似于prompt_input，但适用于无须输入的情况。
- **response_split**：定义了答案的分隔符。

例如：
```json
{
    "description": "Template used by Med Instruction Tuning",
    "prompt_input": "下面是一个问题，运用医学知识来正确回答提问。\n### 问题:\n{instruction}\n### 回答:\n",
    "prompt_no_input": "下面是一个问题，运用医学知识来正确回答提问。\n### 问题:\n{instruction}\n### 回答:\n",
    "response_split": "### 回答:"
}
```

#### 10.3.5 LoRA微调主要参数配置
微调是指在预训练模型的基础上，通过使用特定任务的数据集进行额外微调以提升模型在该任务上的性能。在微调过程中，需要配置一些关键参数以确保微调的顺利进行和性能达到最优。下面是在“finetune.py”文件的微调代码中的主要参数配置。
1. **微调相关参数修改**
    - **--base_model**：设置为所选LLM的名称，如Qwen1.5 - 0.5B。读者可以在Hugging Face官方网站上查询更多模型信息。
    - **--data_path**：微调数据集的路径。
    - **--output_dir**：微调后模型的保存路径。
    - **--prompt_template_name**：提示词模板的名称，根据项目需求进行内容调整。
    - **--batch_size**：微调时的数据批处理大小，根据计算机算力进行调整。
    - **--micro_batch_size**：用于将大批次（batch_size）拆分为多少个小批次，以减少显存占用并实现梯度累加，从而优化GPU资源的使用。
    - **--wandb_run_name**：指定在Weights and Biases（wandb）上记录的运行名称。

在项目代码中进行微调参数设置的步骤如下所示。

第一步：在PyCharm中，选择右上角的“Edit Configurations”选项，如图10 - 40所示。

第二步：在“Run/Debug Configurations”界面中单击“Parameters”输入框，并在输入框中输入图10 - 41所示的配置信息。


![image](https://github.com/user-attachments/assets/fd64451e-cf56-4856-8d41-488c96380f64)


![image](https://github.com/user-attachments/assets/a3734289-c9f4-470f-927d-af4a009e2c13)


2. **LoRA微调参数配置**

在微调过程中，使用LoRA方法进行微调的参数配置如下所示。
    - **lora_r**：LoRA微调的秩。
    - **lora_alpha**：影响放大倍数。
    - **lora_dropout**：微调中丢弃的概率。
    - **lora_target_modules**：LoRA需要微调的参数列表。

3. **微调训练参数配置**

在配置Trainer对象时，需要配置一系列参数，主要包括如下几个。
    - **num_train_epochs**：微调的总轮数。
    - **learning_rate**：学习率。
    - **optim**：优化器，如adamw_torch。
    - **fp16**：是否使用混合精度进行微调。

#### 10.3.6 微调主要执行流程

10.3.5节介绍了根据任务需求配置LoRA微调主要参数的内容，下面介绍微调过程中主要使用的方法和函数，以及微调流程。

（1）首先，需要引入所需的分词器和模型，以便后续使用：
```python
from transformers import Qwen2Tokenizer, Qwen2ForCausalLM
```
（2）加载模型。



使用from_pretrained方法加载预训练模型，并设置相关参数：
```python
model = Qwen2ForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.float32,
    device_map=device_map
)
```
（3）加载分词器。


使用from_pretrained方法加载预训练模型对应的分词器，用于对输入进行分词处理：
```python
tokenizer = Qwen2Tokenizer.from_pretrained(base_model)
```
（4）加载微调参数。


使用LoRA微调参数配置模型：
```python
config = LoraConfig(
    r=lora_r,
    lora_alpha=lora_alpha,
    target_modules=lora_target_modules,
    lora_dropout=lora_dropout,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, config)
``` 
