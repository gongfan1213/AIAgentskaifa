### 第11章 Llama 3模型的微调、量化、部署和应用
#### 图11-11（界面截图）


![image](https://github.com/user-attachments/assets/7316a6c9-d7bb-43b7-9356-51ff20a34429)


### 4. 数据集配置
#### 1）数据集文件的格式
在使用Llama - factory或其他机器学习框架进行模型微调时，数据集文件的格式是非常重要的。常见的数据集文件格式包括CSV（Comma - Separated Values）、JSON（JavaScript Object Notation）、TSV（Tab - Separated Values），以及Hugging Face的Dataset等。下面详细介绍这些格式及其特点。
- **CSV**：以逗号分隔的纯文本文件。每一行代表一个数据样本，每一列代表一个特征。简单易读，适用于结构化数据。
- **JSON**：以键/值对存储数据，支持嵌套结构，可读性好，适用于结构复杂的数据，被广泛应用于API和配置文件中。 
- **TSV**：以制表符分隔的纯文本文件。类似于CSV，但其使用制表符分隔，避免了数据中包含逗号的问题。 
- **Dataset格式**：专为机器学习设计，支持多种数据操作，直接与Hugging Face的datasets库兼容。其支持从多种数据源（CSV、JSON、TSV等）加载数据，并提供了高效的数据处理和转换工具。 
#### 2）数据集下载
准备工作中已下载一份Hugging Face的公开数据集，下载JSON文件后，修改文件名为自己想要的，如alpaca_zh_demo.json。 
#### 3）数据集修改及配置
打开下载好的数据集，其数据格式比较简单，用户可根据具体需求对内容进行适当调整，如图11 - 12所示。
```json
[
    {
        "instruction": "识别并解释给定列表中的两个科学理论：细胞理论和日心说。",
        "input": "",
        "output": "细胞理论是生物学的一个理论，它认为所有生命体都是由最小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成的，细胞只能通过细胞分裂产生新的细胞。这一理论由施莱登、施旺和鲁道夫·菲尔绍于1839年首次提出。日心说是指太阳是太阳系的中心，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，为天文学的发展做出了巨大贡献。"
    },
    {
        "instruction": "输入三支篮球队的名称并生成一个适当的口号。",
        "input": "输入：俄克拉荷马城雷霆队、芝加哥公牛队、布鲁克林篮网队。",
        "output": "输出：雷霆、公牛和篮网！奋勇冲刺，角逐群雄！"
    },
    {
        "instruction": "比较以下两位著名人物，使用特征列表并以句子形式提供输出。",
        "input": "人物：阿尔伯特·爱因斯坦 vs 人物：斯蒂芬·霍金",
        "output": "人物：阿尔伯特·爱因斯坦 vs 人物：斯蒂芬·霍金特征：智力、教育、对世界的影响爱因斯坦：拥有超凡的智力，接受过高等教育，他推进了科学发展并在世界范围内产生了深远的影响。霍金以其相对论和黑洞相关公式而闻名，而霍金以其关于黑洞和宇宙的发现而闻名。两位科学家都以其深厚的学识和非凡的贡献影响了世界。"
    }
]
```

![image](https://github.com/user-attachments/assets/90963af8-6d25-4000-b3c7-d385bea0f50c)


数据集修改完成后，将其保存为JSON格式，为了让Llama - factory能够加载该数据集，需要在数据集配置文件data/dataset_info.json中添加该数据集选项，如图11 - 13所示。
```json
{
    "alpaca_zh_demo": {
        "file_name": "alpaca_zh_demo.json"
    },
    "dataset_info.json": {
        "identity": {
            "file_name": "identity.json"
        },
        "alpaca_zh_demo": {
            "file_name": "alpaca_zh_demo.json"
        },
        "glaiver_toolcall_en_demo": {
            "file_name": "glaiver_toolcall_en_demo.json",
            "formatting": "sharegpt",
            "columns": {
                "messages": "conversations",
                "tools": "tools"
            }
        }
    }
}
```


![image](https://github.com/user-attachments/assets/5e41d421-8cba-434e-8a38-b439161e6ce7)


添加完选项后保存文件，即可在微调参数中进行选择。 
#### 4）配置微调参数
Llama3 - 8B原版模型在未进行微调之前，对中文的支持非常不友好，可以说基本不支持中文，如图11 - 14所示。
```
你是谁
OI Llama3:Latest 300502024_09_19
I'm Llama, a large language model trained by a team of researcher at Meta AI. I'm a type of AI
designed to generate human - like text responses to user input. My training data includes a
massive corpus of text from various sources, which allows me to understand and respond to
natural language inputs.
In simple terms, I'm an AI chatbot that can have conversations with humans in a way that's
similar to how humans talk to each other. I'm constantly learning and improving my responses
based on the interactions I have with users like you!
So, what would you like to talk about?
```


![image](https://github.com/user-attachments/assets/906e1cb5-3128-43e4-ac28-5e9ef7da2d47)


本次微调的目标是通过微调使新的Llama 3模型具备一定的中文理解和推理能力。
- **语言**：选择zh。
- **模型名称**：选择Llama3 - 8B。
- **模型路径**：选择/opt/Llama3 - 8B（按照自己模型的路径位置进行配置）。 
- **微调方法**：选择lora。 
- **适配器路径**：无须选择，微调后会自动生成相应的适配内容。 
- **训练阶段**：选择Supervised Fine - Tuning。 
- **数据路径**：选择data（根据自己的数据集配置文件进行选择）。 
- **数据集**：选择alpaca_zh_demo（根据自己所需数据集进行选择）。 
- **学习率**：设定为2e - 4。 
- **训练轮数**：设定为10。 
- **最大样本数**：设定为1000。 
其他参数暂不设置，使用默认设置即可，学习率、训练轮数、最大样本数、批处理大小等参数对模型训练结果有重要影响，此处只介绍模型微调流程，详细的模型微调内容请参考《机器学习方法》《参数高效微调方法综述》等书籍和文章。 
### 5. 执行微调过程
参数配置完成后，单击页面下方的“开始”按钮，启动微调，如图11 - 15所示。


![image](https://github.com/user-attachments/assets/8e41bbe4-6fa8-484b-b80a-3dbc5030da55)


#### 图11-15（微调参数设置界面截图及日志信息）
后台同步启动，如图11 - 16所示。


![image](https://github.com/user-attachments/assets/ffd168fb-64d0-4822-9ae5-6302ffe2bb94)


#### 图11-16（后台启动日志信息截图）
加载基础模型，GPU显存占用巨大，如图11 - 17所示。


![image](https://github.com/user-attachments/assets/e7644e33-cf58-47a6-ada7-c81ed5975ccf)


#### 图11-17（NVIDIA - SM信息截图，显示GPU相关信息及进程占用情况）


![image](https://github.com/user-attachments/assets/308930e3-1042-45e3-8ee3-d189ba5175e7)


加载底模后，开始执行模型微调过程，如图11 - 18所示。
#### 图11-18（微调过程界面截图，有日志信息和损失折线图区域）


![image](https://github.com/user-attachments/assets/111590ee-1630-4ea1-ba10-0aefdbd77b8c)


随着训练步骤的开展，TensorFlow生成了损失折线图，如图11 - 19所示。
#### 图11-19（损失折线图，有original和smoothed两条曲线）
微调过程持续执行，如图11 - 20所示。


![image](https://github.com/user-attachments/assets/f9c793ab-9810-4011-bbc2-2798447da5f2)


#### 图11-20（微调持续执行界面截图，有日志和损失折线图）
训练完成后的页面如图11 - 21所示。



![image](https://github.com/user-attachments/assets/8d10bdd0-5c29-42a7-86b0-1b901d5ce01c)


#### 图11-21（训练完成后界面截图，有参数设置等区域）
加载训练后的微调模型，可正常进行对话，并且该模型对于中文具备一定的理解能力，能够正常推理生成基本符合期望的答案，如图11 - 22和图11 - 23所示。


![image](https://github.com/user-attachments/assets/1f3d994d-744c-49dd-8d52-566a415b3594)


#### 图11-22（加载微调模型界面截图）


![image](https://github.com/user-attachments/assets/81e4708b-bb4c-4d91-bff5-9d5b7278a464)


#### 图11-23（模型对话界面截图，有输入输出内容）
导出模型，如图11 - 24所示。


![image](https://github.com/user-attachments/assets/e8bd8081-0c33-487f-be19-c14798fa7908)


#### 图11-24（导出模型界面截图）
### 6. 损失折线图在模型训练中的意义和参考点
损失折线图是模型训练过程中一款非常重要的可视化工具。它展示了训练和验证阶段的损失值随时间（通常是随训练轮数或步骤）的变化情况。理解和分析损失折线图可以帮助我们评估模型的性能和训练效果，及时发现并纠正训练过程中存在的问题。以下是损失折线图在模型训练中的主要意义和参考点。
#### （1）监控模型的收敛情况。
- **收敛**：如果损失值随着训练的进行不断降低，并且趋近于某个稳定值，则表明模型正在学习，并逐渐逼近最佳解。在这种情况下，损失折线图会显示出一条下降并逐渐平稳的曲线。 
- **未收敛**：如果损失值在训练过程中没有明显下降，或者波动很大，则表明模型可能没有学习到有效的特征，此时需要调整超参数或优化方法。 
#### （2）识别过拟合和欠拟合。
- **过拟合**：如果训练损失值持续降低，但验证损失值在降低到一定程度后开始上升，则说明模型在训练集上表现很好，但在验证集上表现不佳。这种情况通常表现为训练损失曲线持续下降，而验证损失曲线先下降后上升。 
- **欠拟合**：如果训练损失值和验证损失值都保持在较高水平，并且没有明显下降，则说明模型的复杂度不够，无法有效捕捉数据中的模式。此时，损失折线图会显示出两条曲线都保持在高位且几乎没有变化的情况。 
#### （3）调整学习率和其他超参数。
- **学习率**：如果训练损失曲线下降很慢或者出现波动，则可能需要调整学习率。学习率过高会导致训练不稳定，曲线波动较大；学习率过低则会导致收敛速度放慢，曲线下降缓慢。 
- **批处理大小**：设置适当的批处理大小可以加快训练速度并提升模型性能。批处理大小设置得过大或过小，都会影响训练损失曲线的变化情况。 
#### （4）提供训练进度的实时反馈。
损失折线图能够实时反映模型的训练进度，帮助用户在训练过程中及时做出调整。例如，如果看到训练损失曲线在某个阶段突然增加，则可以检查数据集是否发生变化，或模型参数是否出现问题。 
**示例分析**：
假设有如下损失折线图。
- 曲线A：不断下降，趋于平稳，表明模型在训练集上表现良好。 
- 曲线B：先下降后上升，表明模型开始过拟合，需要采取相应措施（如正则化或早停）。 
- 曲线C：一直保持在高位，表明模型欠拟合，需要增加模型复杂度或调整超参数。 
### 11.3 模型量化
#### 11.3.1 量化的概念与优势
模型量化是指将模型的权重和激活值转换为低精度（如Int8）格式的过程。量化的主要优势包括模型压缩与加速，以及部署成本的降低。
### 1. 模型压缩与加速
量化通过减少模型参数的位数，可以显著压缩模型的大小，使模型占用的存储空间大幅度减小。同时，低精度模型的计算速度在现代硬件上通常比高精度模型的计算速度更快，因此量化也能加速模型的推理过程。 
### 2. 部署成本降低
通过模型量化，模型在推理时所需的内存和计算资源大幅减少，这不仅提高了模型的运行效率，还降低了部署成本，特别是在资源受限的设备（如移动设备和嵌入式系统）上，量化后的模型能够更高效地运行。 
#### 11.3.2 量化工具Llama.cpp介绍
Llama.cpp是一个用于高效推理和部署LLM（如Llama模型）的项目。该项目通过使用多种量化方法显著降低了模型的计算复杂度和存储需求，从而实现高效推理。以下是Llama.cpp使用的主要量化方法。
### 1. 8 - bit和4 - bit定点量化
8 - bit和4 - bit量化是Llama.cpp中常用的量化方法。这些方法通过将模型的权重和激活值从32位浮点数转换为8位或4位整数，从而显著降低模型的计算复杂度和存储需求，具体技术如下。
- **权重量化**：将模型的权重从32位浮点数转换为更低精度的8位或4位整数。这种方法大大降低了模型的存储需求。 
- **激活量化**：类似地，将激活值从32位浮点数转换为8位或4位整数，在推理过程中使用定点算术运算，从而提高计算效率。 
### 2. 混合精度量化
混合精度量化方法结合了高精度和低精度计算的优点，对不同层或不同类型的操作使用不同的精度，以达到最佳的性能和精度平衡。例如，对计算要求较高的层使用较高精度（如8位），对计算要求较低的层使用较低精度（如4位）。 
### 3. 动态范围量化
动态范围量化通过在推理时动态调整权重和激活值的范围来减少量化误差。这种方法允许模型在不同的输入数据范围内自适应调整，从而进一步降低量化带来的精度损失。 
### 4. 梯度量化
在训练过程中，梯度量化通过量化反向传播过程中的梯度值来降低梯度存储和计算的需求。这种方法在保证模型训练精度的同时，提高了训练效率和可扩展性。 
### 5. 矩阵量化
矩阵量化方法常用于对大规模矩阵乘法运算的优化，通过对矩阵进行分块或低秩近似来实现量化，特别适用于Transformer等复杂模型，具体技术如下。
- **分块量化**：将大矩阵分割成小块，对每个小块进行独立量化。 
- **低秩近似量化**：通过低秩矩阵分解来逼近原始矩阵，从而实现高效的量化表示。 
#### 11.3.3 Llama.cpp部署
### 1. 环境准备
安装git及cmake工具，如图11 - 25所示。 （此处未显示图11 - 25内容） 
