### 第7章 RAG应用案例——使用RAG部署本地知识库
假如现在问大语言模型Llama 3一个问题：当你在创建投资模型时，这些模型是否明确考虑了货币环境等因素，包括美国联邦储备系统①的行动？还是仅仅依赖于统计分析？

大语言模型Llama 3的回答如图7-1所示。


![image](https://github.com/user-attachments/assets/3970b253-85f1-4fd4-9689-73a91854f69f)


**翻译**
- **我们会明确将宏观金融因素纳入考量**：在开发我们的投资模型时，我们有意识地考虑了一系列影响市场行为的宏观金融因素。这包括货币环境，例如美联储（Fed）等中央银行的行动和决策。我们的模型旨在纳入这些外部因素，以提供更准确和稳健的预测。
- **理解市场之间的相互作用**：通过考虑美联储的政策决策及其对利率、通货膨胀、就业和其他关键经济指标的影响，我们可以更好地预测这些因素将如何影响资产价格。这使我们能够改进我们的模型并在考虑市场之间复杂的相互作用的情况下做出更加明智的投资决策。
- **超越统计分析**：虽然统计分析对于构建可靠的模型至关重要，但它本身是不够的。我们的方法结合了定量和定性要素，创建了一个全面的框架，考虑到了各种市场驱动因素之间复杂的关系。这使我们能够生成更准确的预测，并在不断变化的货币条件下做出更明智的投资决策。
- **综上所述**：我们的投资模型确实考虑了像美联储行动这样的宏观金融因素，以提供对市场行为更细致的理解，并指导我们的投资策略。

图7-1所示内容是大语言模型Llama 3根据之前训练习得的知识进行的回答。
现在让大语言模型Llama 3读取一个采访James Simons（量化交易鼻祖，文艺复兴科技公司的创始人）的PDF文档后，再问Llama 3同样的问题，其回答如图7-2所示。

**翻译**

**RAGit的回答**：据我所知，由James Simons和他的团队在文艺复兴科技公司开发的Medallion基金的投资模型主要侧重于数学和统计分析。据James Simons所说，他们的方法就是“坚持数学”，依赖这些工具的预测，而不是考虑其他流行的趋势或偏见。

虽然这些模型确实考虑了市场状况和宏观经济因素，但它们并没有明确纳入美联储（Fed）的货币政策决策信息。其重点更多的是使用先进的数学和统计技术来识别金融市场中的模式和异常。

换句话说，Medallion基金的投资模型主要是围绕统计分析和机器学习算法构建的，而不是依赖于对宏观经济或货币政策因素的明确考虑。


可以看到，后者明显是结合了PDF文档中对James采访的上下文来作答的，PDF文档中的采访节选如图7-3所示。


![image](https://github.com/user-attachments/assets/5d8815d3-cd8d-4b66-a886-1215cec49947)


![image](https://github.com/user-attachments/assets/e67edc24-bda1-4ed0-8030-5cbfbd8ce1f3)


**翻译**

James Simons：我们用统计学和数学来形式化我们对货币环境等事物的理解。

这印证了RAG技术就是大语言模型的逻辑推理能力+自定义的知识库。

通过上面的例子可知，如果想让大语言模型有针对性地回答我们提出的问题，则需要给大语言模型定制一个知识库。下面使用RAG部署本地知识库，从而让大语言模型可以更精准地回答我们提出的问题。

整体流程如图7-4所示。


![image](https://github.com/user-attachments/assets/d577f52e-bcd1-4a18-9152-d92327648544)


#### 7.1 部署本地环境及安装数据库

**7.1.1 在Python环境中创建虚拟环境并安装所需的库**

首先在Python环境中创建虚拟环境，以避免出现版本冲突、方便部署。

requirements.txt位于phidata库的cookbook/llms/ollama/rag/目录中，包含了目前RAG项目所需的所有库。因为之后需要使用requirements.txt进行批量库的安装，所以先创建虚拟环境，以免对其他Python项目或者现有库造成影响。

打开命令提示符界面或“powershell”界面，输入以下命令，创建名为“venv”的虚拟环境（可以根据你的项目来命名）：
```bash
Python -m venv //venv为自定义名称
```
输入以下命令，激活虚拟环境：
```bash
venv\scripts\activate
```
输入以下命令，在虚拟环境中安装所需的库（/requirements.txt前要加文件所在路径）：
```bash
pip install -r cookbook/llms/ollama/rag/requirements.txt
```
等待项目所需的库批量安装完成后，就可以进行下一步操作了。

**7.1.2 安装phidata库**

phidata是一个开源框架，旨在帮助开发者构建具有长期记忆、上下文知识，以及使用函数调用执行动作能力的自主RAG助手。在本例中，利用phidata库来配置本地RAG框架，包括已配置好的向量数据库（pgvector）。RAG的基本架构如图7-5所示。

phidata的目标是：将通用的LLM转变为针对特定用例的专业化RAG助手。

其解决方案是通过记忆、知识和工具扩展LLM：

- **记忆**：在数据库中存储聊天记录，使LLM能够进行长期对话。
- **知识**：在向量数据库中存储信息，为LLM提供上下文知识。
- **工具**：使LLM能够执行从API获取数据、发送邮件或发送请求等动作。


![image](https://github.com/user-attachments/assets/4e353aa4-ff8a-4ffa-b27a-67411defa0c7)


phidata库的安装方法有如下两种，具体安装过程不再介绍。

- **方法一**：在Python环境中通过输入“pip install -U phidata”命令进行安装。
- **方法二**：在GitHub中通过搜索“phidata”下载并安装。

**7.1.3 安装和配置Ollama**

Ollama是一款开源的大语言模型服务工具，允许用户在本地环境中轻松运行和管理大语言模型。在本项目中，我们使用的大语言模型Llama 3、embedding（嵌入文本）模型nomic-embed-text就是通过Ollama来安装的。

1. **下载和安装Ollama**

访问Ollama官方网站，进入下载页面，根据你的计算机所使用的操作系统（Windows、Linux或macOS）选择相应的安装包，单击“下载”按钮下载安装包。下载完成后，双击安装包并按照提示进行安装即可。

2. **配置Ollama环境变量**

（1）在Windows“开始”菜单中选择“设置”命令，在上方“查找设置”搜索框中输入“环境变量”并按回车键。

（2）选择“搜索结果”界面中的“编辑系统环境变量”选项。

（3）在弹出的“系统属性”对话框中，单击“高级”选项卡中的“环境变量”按钮。

（4）找到“Path”选项并双击或者单击“编辑”下的“新建”按钮。

如果采用系统默认安装路径，则复制C:\Users\Administrator\AppData\Local\Programs\Ollama路径，或者寻找ollama.exe的安装位置，复制路径并粘贴后，单击“确定”按钮。



**7.1.4 基于Ollama安装Llama 3模型和nomic-embed-text模型**

大语言模型，以众所周知的GPT为例，是经过训练学习后，有一定逻辑推理能力、可实现对话交互的模型。本例采用的大语言模型为Llama 3。

embedding模型是一种将高维数据（如单词、句子、图像等）转换为低维向量的模型。这种表示方法使得复杂的数据可以在向量空间中进行数学运算，便于计算和分析。embedding模型被广泛应用于自然语言处理、计算机视觉（CV）和推荐系统等领域。其基本思想是将数据表示为向量，使得相似的对象在向量空间中距离较近，不相似的对象在向量空间中距离较远。通过这种方式，可以使用向量空间中的几何性质来进行各种计算和操作。本例采用的embedding模型为nomic-embed-text。

本例需要基于Ollama安装Llama 3模型和nomic-embed-text模型，相关操作如下。

配置好Ollama的环境变量后，打开“powershell”界面，首先输入“ollama pull llama3”命令安装Llama 3模型。然后输入“ollama pull nomic-embed-text”命令安装nomic-embed-text模型。安装成功后，通过输入“ollama list”命令可以看到已安装好的模型。
```
nomic-embed-text:latest
llama3:latest
```


**7.1.5 下载和安装Docker并用Docker下载向量数据库的镜像**

Docker作为容器，可以存放配置好的向量数据库。Docker允许开发者和系统管理员将应用程序及其依赖打包到一个可移植的容器中，并在任何支持Docker的系统上运行。容器化是一种轻量级的虚拟化技术，允许应用程序在隔离的环境中运行，而不需要完整的操作系统。在本例中，使用Docker下载向量数据库的镜像。

（1）首先，打开Docker官方网站，下载和安装Docker。然后，打开“powershell”界面，输入“docker pull phidata/pgvector:16”命令，从Docker中下载向量数据库的镜像。

接着，输入“docker run -d -e POSTGRES_DB=ai -e POSTGRES_USER=ai -e POSTGRES_PASSWORD=ai -e PGDATA=/var/lib/postgresql/data/pgdata -v pgvolume:/var/lib/postgresql/data -p 5532:5432 --name pgvector phidata/pgvector:16”命令，即可在向量数据库中，完成账号和密码创建、端口设置等一系列操作，这时“powershell”界面中会显示一长串字符，表示操作已完成。

（2）检查Docker中的向量数据库是否已启动。

完成第（1）步操作后，Docker中的向量数据库会自动启动，打开Docker会看到已安装的向量数据库，如图7-6所示，可以看到容器图标“ ”，其“Actions”栏显示的是“启动”图标，表示向量数据库已启动（如果关机或重启，则打开Docker，单击“启动”图标即可启动向量数据库）。


![image](https://github.com/user-attachments/assets/7139a874-3cdd-457c-9ec8-a6b841c7394f)


#### 7.2 代码部分及前端展示配置

7.1节介绍了大语言模型、embedding模型的安装及向量数据库的下载操作，现在需要编写代码来实现与大语言模型的页面交互。代码共分为两个Python程序文件，分别命名为app.py和assistant.py。

（1）app.py作为应用程序，通过streamlit库的指令进行调用启动。

（2）assistant.py中主要定义了get_rag_assistant函数，由app.py调用。

（3）get_rag_assistant函数主要用于对Assistant类进行设置。


**7.2.1 assistant.py代码**
1. **导入所需库**
```python
from typing import Optional
from phi.assistant import Assistant
from phi.knowledge import AssistantKnowledge
from phi.llm.ollama import Ollama
from phi.embedder.ollama import OllamaEmbedder
from phi.vectordb.pgvector import PgVector2
from phi.storage.assistant.postgres import PgAssistantStorage
```


2. **将Docker中向量数据库的URL赋值给参数db_url**

```python
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
```

3. **get_rag_assistant函数**

```python
def get_rag_assistant(
    llm_model: str = "llama3",
    embeddings_model: str = "nomic-embed-text",
    user_id: Optional[str] = None,
    run_id: Optional[str] = None,
    debug_mode: bool = True,
) -> Assistant:
    embedder = OllamaEmbedder(model=embeddings_model, dimensions=4096)
    embeddings_model_clean = embeddings_model.replace("-", "_")
    if embeddings_model == "nomic-embed-text":
        embedder = OllamaEmbedder(model=embeddings_model, dimensions=768)
    elif embeddings_model == "phi3":
        embedder = OllamaEmbedder(model=embeddings_model, dimensions=3072)
    elif embeddings_model == "shunyue/llama3-chinese-shunyue":
        embedder = OllamaEmbedder(model=embeddings_model, dimensions=768)
    knowledge = AssistantKnowledge(
        vector_db=PgVector2(
            db_url=db_url,
            collection=f"local_rag_documents_{embeddings_model_clean}",
            embedder=embedder,
        ),
        num_documents=3,
    )

    return Assistant(
        name="local_rag_assistant",
        run_id=run_id,
        user_id=user_id,
        llm=Ollama(model=llm_model),
        storage=PgAssistantStorage(table_name="local_rag_assistant", db_url=db_url),
        knowledge_base=knowledge,
        description="You are an AI called 'RAGit' and your task is to answer questions using the provided information",
        instructions=[
            "Do not use phrases like 'based on my knowledge' or 'depending on the information'.",
            "When a user asks a question, you will be provided with information about the question.",
            "Carefully read this information and provide a clear and concise answer to the user.",
            "add_references_to_prompt=True,",
            "markdown=True,",
            "add_datetime_to_instructions=True,",
            "debug_mode=debug_mode,",
        ],
    )
```

**代码解释如下**：

（1）**初始化组件**。

Assistant类：这是RAG系统的主类，负责协调检索和生成过程。

AssistantKnowledge类：用于管理知识库，包括向向量数据库的连接和文档的检索。

OllamaEmbedder类：用于将文本转换为向量，以便在向量数据库中进行检索。

PgVector2类：用于连接和操作PostgreSQL，该数据库使用pgvector扩展来存储和检索向量。

（2）**配置参数**。

llm_model：指定用于生成答案的大语言模型，这里默认为Llama 3。

embeddings_model：指定用于文本嵌入的模型，这里可以是nomic-embed-text、phi3等。

user_id和run_id：用于标识用户和运行实例。

debug_mode：用于控制调试信息的输出。

（3）**知识库设置**。

knowledge_base：通过AssistantKnowledge类初始化，它包含了向量数据库的连接信息和文档集合。

vector_db：使用PgVector2类连接到PostgreSQL，并指定集合名称和嵌入器。

num_documents：设置在生成答案时考虑的文档数量。

（4）**Assistant类配置**。

name：设置RAG系统的名称。

storage：使用PgAssistantStorage类来管理存储在PostgreSQL中的会话历史。

knowledge_base：设置知识库。

description、instructions：提供给模型的描述和指令，用于指导模型的行为。

add_references_to_prompt：参数值设置为True，表示在用户提示中加入知识库的引用。

markdown：参数值设置为True，表示使用Markdown格式化消息。

add_datetime_to_instructions：参数值设置为True，表示在指令中添加日期和时间。

debug_mode：参数值设置为True，表示启用调试模式。



**7.2.2 app.py代码**

app.py代码的主要功能模块如下。


（1）导入所需库和模块：包括streamlit库、自定义的phi模块中的Assistant、Document、PDFReader、WebsiteReader，以及日志记录工具logger。

（2）设置streamlit页面：通过st.set_page_config设置页面标题和图标。

（3）定义“重启RAG助手”功能的函数：restart_assistant函数用于重置会话状态，包括删除当前的RAG助手实例和重新运行应用程序。

（4）设置主函数main：主函数main是应用程序的核心，负责处理用户输入、选择模型、加载知识库、展示对话历史、生成答案及管理知识库的更新。

以下是代码详细介绍。

1. **导入所需库和模块**

```python
from typing import List
import streamlit as st
from phi.assistant import Assistant
from phi.document import Document
from phi.document.reader.pdf import PDFReader
from phi.document.reader.website import WebsiteReader
from phi.utils.log import logger
from assistant import get_rag_assistant  # type: ignore
```


2. **设置streamlit页面**

```python
st.set_page_config(
    page_title="Local RAG",
    page_icon=":orange_heart:",
)
st.title("迪哥的Agent之Local Rag")
st.markdown("#### :orange_heart: 这个案例用于演示Llama 3应用与向量化检索")
``` 
