![image](https://github.com/user-attachments/assets/010f16e7-7081-4889-9a22-8b25293e6ea4)


下载完成后，回到LM Studio，单击左侧导航栏中的“My Models”图标，进行Local models folder的路径配置，需要在lm - studio\models路径下按照Qwen/Qwen1.5 - 0.5B - Chat - GGUF逐层创建文件夹，这样下载的Qwen/Qwen1.5 - 0.5B - Chat - GGUF文件在lm - studio\models\Qwen\Qwen1.5 - 0.5B - Chat - GGUF路径下才能被LM Studio识别到，如图4 - 46所示。 

![image](https://github.com/user-attachments/assets/3204a7fa-4a5e-4937-8434-005bd15c2906)


模型加载成功后，单击左侧导航栏中的“AI Chat”图标，在界面顶部选择刚刚加载好的通义千问大语言模型。加载后可以测试一下模型是否加载成功，在对话框中输入问题，若可以得到对应的答案，则说明已加载成功，如图4 - 47所示。 

![image](https://github.com/user-attachments/assets/b2cd6a83-6ed4-489a-bb48-5775f0ca6324)


LM Studio右侧的设置项较为丰富，读者可以进一步对GPU算力、提示词等进行个性化设置。

测试成功后，单击LM Studio左侧导航栏中的“Local Server”图标，并单击“Start Server”按钮启动服务。为了进一步验证服务是否启动成功，可以选择“chat（python）”选项，并单击“Copy Code”按钮复制测试代码，结果如图4 - 48所示。 

打开PyCharm IDE，新建项目，并新建Python文件，将其命名为“test”，粘贴刚刚复制的代码，单击“运行”图标。

报错提示需要安装OpenAI，单击左下角的“终端”图标 ，输入“pip install openai”命令进行安装。 

![image](https://github.com/user-attachments/assets/39e73d6a-1e0c-457b-9c66-1caebaada1d9)


安装完成后，再次单击“运行”图标，若返回“ChatCompletionMessage(content="My name is Alex, and I'm a software engineer living in New York City. I enjoy solving problems and creating innovative solutions to real - world challenges. Whether it's coding my way through a complex project or working on a challenging new feature, I always strive to be the best at what I do.", role='assistant', function_call=None, tool_calls=None)”，则表示服务启动成功。

接下来，返回LM Studio，可以看到Server logs也打印了同样的返回结果，说明本地服务已经成功启动，如图4 - 49所示。 

![image](https://github.com/user-attachments/assets/65d78a84-1eed-4ab0-b256-dc3269ebcabd)


返回AutoGen Studio，我们需要将刚刚创建好的本地大语言模型加载进AutoGen Studio的Models中，在这里需要先对Anaconda下的\Lib\site - packages\openai\_client.py文件进行源代码的修改。由于AutoGen Studio目前的版本还不够稳定，在加载本地大语言模型时经常会出现掉线等情况，因此我们需要在_client.py文件的第112行代码后增加如下两行代码：
```python
base_url="http://127.0.0.1:1234/v1"
api_key="lm - studio"
```
修改代码并保存后，选择AutoGen Studio界面的“Build”选项卡下的“Models”选项，并单击“+New Model”按钮，在弹出的对话框中输入模型名称、API Key和Base URL，单击“Test Model”按钮，测试成功后，保存当前设置，如图4 - 50所示。 

![image](https://github.com/user-attachments/assets/b6af22cd-eb22-44d2-8def-910fcf893557)



接下来，选择“Build”选项卡下的“Workflows”选项，创建一个使用本地大语言模型的工作流，将其命名为qwen，只需配置好“Model”选项即可，如图4 - 51所示。 

![image](https://github.com/user-attachments/assets/77d7fbd5-3dab-419c-88d7-8fa635957c73)


切换到“Playground”选项卡，单击“+New”按钮，加载刚刚创建好的qwen工作流，在对话框中输入“你是谁”，回答“我是来自阿里云的大语言模型，我叫通义千问。”，完成调试，如图4 - 52和图4 - 53所示。 

![image](https://github.com/user-attachments/assets/4344eb5c-84ae-4ee6-a347-b2420182a332)

![image](https://github.com/user-attachments/assets/23c91498-3ffb-450b-a94b-70fdbb93ae3d)


### 下面在本地部署Ollama工具，使读者对在本地部署大语言模型服务有更进一步的理解。

![image](https://github.com/user-attachments/assets/7e7ab419-0131-4a15-bc66-d5876ec8ae48)


打开Ollama官方网站，单击“Download for Windows (Preview)”链接下载安装包，下载完成后，双击Ollama安装包进行安装。安装完成后，读者可以选择适合自己计算机配置的模型进行运行，如图4 - 54所示。 

打开命令提示符（cmd）界面，输入“ollama run llama3”命令，等待拉取完成，并通过输入“你是谁”进行测试，如图4 - 55所示。 

![image](https://github.com/user-attachments/assets/f7c290ad-ec03-4f92-8cb5-aa4f9927d1c7)


再打开一个命令提示符界面，输入“pip install litellm”命令，安装litellm，如图4 - 56所示。litellm可以调用所有LLM API（如Bedrock、Huggingface、VertexAI、TogetherAI、Azure、OpenAI等）。 

![image](https://github.com/user-attachments/assets/c8279fb1-c74b-4ddd-8c76-ff8b3ecc7721)


litellm安装完成后，输入“litellm --model ollama/llama3”命令，如图4 - 57所示。 

![image](https://github.com/user-attachments/assets/879a1044-4f39-4012-8313-98af7089483d)


如果遇到问题，则根据提示安装相关的依赖包，直到启动服务为止，如图4 - 58所示。 

![image](https://github.com/user-attachments/assets/439107a9-02fe-4167-9541-61d3cdd8dc10)


下面回到AutoGen Studio界面，选择“Build”选项卡下的“Models”选项，并单击“+New Model”按钮，将名称修改为“Llama 3”，指定Base URL为“http://0.0.0.0:4000”，单击“Save”按钮进行保存。
最后，创建工作流，并在“Playground”选项卡中新建Llama 3，完成调试。 

### 2. AutoGen Studio本地化服务部署
AutoGen Studio本地化服务部署需要将上面使用的AutoGen Studio的用户界面封装成服务，以便用户请求调用。下面将通过一个实际案例带领读者，一边演示一边理解封装服务。

案例介绍：迪哥经常为运营自己的公众号烦恼，他需要花费很多时间先在网上看各种视频，再把视频内容总结出来发布到公众号里。因此，迪哥想通过AutoGen Studio制作一个根据输入的视频地址，自动获取视频内容，并将其生成为公众号文章的Agent。通过这个Agent来帮助迪哥提高工作效率，节省大量时间。

第一步，迪哥已经配置好此次部署Agent的相关环境，如图4 - 59所示。 

![image](https://github.com/user-attachments/assets/5f28e1a8-9ab7-48ba-9e03-9afcd9039aac)


Replit是一个在线集成开发环境（IDE），也是一个代码协作平台和云服务提供商。它支持多种编程语言，如Python、JavaScript、Java等，非常适合初学者使用。基于Replit，用户无须安装任何软件，只需通过浏览器即可运行代码、创建项目、与他人协作和共享项目。Replit提供了一系列功能和工具，如代码自动生成、调试器、版本控制和部署工具等，以便用户能够更轻松地进行编程工作。另外，Replit还提供了大量的框架支持，包括React和Flask等，并且可以一键部署GitHub的开源代码。

接下来单击“Fork”按钮复制迪哥的本地化部署代码到自己的仓库中，用户需要提前在Replit平台上注册账号。注册并登录Replit平台后，单击“Fork”按钮，在“Fork Repl”对话框中输入名称和描述，单击“Fork Repl”按钮创建仓库，如图4 - 60所示。 

创建成功后，回到AutoGen Studio，下载之前已经配置好的工作流（如之前已经配置好的MyGroup Workflow），即下载“write.json”文件。 

![image](https://github.com/user-attachments/assets/e629d02e-a50f-4352-b14e-942eb66b14ad)


将“write.json”文件加载到Replit工程中，单击“Run”按钮，启动服务。
“main.py”文件的代码块如下： 
