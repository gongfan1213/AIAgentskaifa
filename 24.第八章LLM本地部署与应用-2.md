### 8.8 通义千问1.5-0.5B本地Windows部署实战
#### 8.8.1 介绍

通义千问1.5-0.5B（Qwen1.5-0.5B）是阿里云研发的通义千问LLM系列的具有0.5B参数规模的模型。Qwen1.5-0.5B是基于Transformer架构的LLM，在超大规模的预训练数据上训练得到。预训练数据类型多样，覆盖面广，包括大量网络文本、专业书籍、代码等。同时，在Qwen1.5-0.5B的基础上，开发人员使用对齐机制打造了基于LLM的AI助手Qwen1.5-0.5B-Chat。本案例使用的仓库为Qwen1.5-0.5B-Chat的仓库。

Qwen1.5-0.5B主要有以下特点：

- **低成本部署**：Qwen1.5-0.5B提供了Int8和Int4量化版本，推理仅需占用不到2GB显存，生成2048个token仅需占用3GB显存，微调最低仅需占用6GB显存。此外，它还提供了基于AWQ、GPTQ、GGUF技术的量化模型。

- **大规模高质量训练语料**：Qwen1.5-0.5B使用超过2.2万亿个token的数据进行预训练，包含高质量多语言、代码、数学等数据，涵盖通用及专业领域的训练语料。通过大量对比实验，Qwen1.5-0.5B对预训练语料分布进行了优化。 

- **优秀的性能**：Qwen1.5-0.5B支持32KB上下文长度，在多个中英文下游评测任务上（涵盖常识推理、代码、数学、翻译等），效果显著超越现有的相近规模开源模型，具体评测结果见下文。 

- **覆盖更全面的词表**：相比目前以中英文词表为主的开源模型，Qwen1.5-0.5B使用了约15万个词表。该词表对多语言更加友好，方便用户在不扩展词表的情况下对部分语种进行功能增强和扩展。 

- **系统指令跟随**：Qwen1.5-0.5B可以通过调整系统指令，实现角色扮演、语言风格迁移、任务设定和行为设定等功能。 



如果读者想了解更多关于Qwen1.5-0.5B开源模型的细节，可参阅GitHub代码库。



#### 8.8.2 环境要求

- **Python**：Python 3.8及以上版本。

- **PyTorch**：PyTorch 1.12及以上版本，推荐使用PyTorch 2.0及以上版本，安装命令如下：

```bash

pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

```

- **建议使用CUDA**：CUDA11.8及以上（GPU用户、flash-attention用户等需考虑此选项）版本。



#### 8.8.3 依赖库安装

运行Qwen1.5-0.5B-Chat，请先确保已满足8.8.2节的环境要求，再执行以下pip命令安装依赖库：

```bash

pip install transformers>=4.37.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft

```

以下是对相关依赖库的介绍。

1. **transformers**

transformers库是由Hugging Face开发的，提供了大量的预训练模型，以及用于执行自然语言处理任务的工具。

该库支持多种Transformer架构，并提供了方便的API用于模型的加载、训练和推理。

在4.37.0或更高版本中，包含了一些新特性，以及性能优化和Bug修复功能，使得模型的应用更加高效和稳定。 

2. **accelerate**

accelerate库旨在简化分布式训练和混合精度训练的设置过程。


它可以帮助用户更容易地在多个GPU或TPU上进行模型训练，并支持自动混合精度训练，以提高训练速度和降低显存使用率。

accelerate库与transformers库可以很好地集成，为用户提供一种无缝的方式来扩展他们的训练工作负载。 

3. **tiktoken**

tiktoken是一个开源的Python模块，实现了高效的tokenizer，特别是BPE（Byte Pair Encoding）算法。

相较于其他tokenizer库，tiktoken在性能上进行了优化，运行速度更快。

tiktoken库为自然语言处理任务中的文本编码提供了高效且灵活的解决方案。 

4. **einops**

einops库提供了一种简洁的语法来操作和重塑张量。


通过使用einops库，用户可以轻松地重新排列、转换和组合多维数组（如PyTorch张量），这在处理神经网络的输入/输出时非常有用。

einops库与transformers库结合使用时，可以简化模型输入/输出数据的处理流程。 

5. **SciPy**

SciPy是一个用于科学和数学计算的Python库。

SciPy库提供了许多高级的数学算法和函数，包括统计、优化、线性代数、积分等。

在自然语言处理任务中，SciPy库可用于执行数据处理、特征提取操作和某些数学计算。 

6. **transformers_stream_generator**

transformers_stream_generator是一个特定版本的工具或库（考虑到其版本号0.0.4），用于生成流式文本输出。

根据名称推测，transformers_stream_generator库与transformers库相关，并提供了一种在推理过程中以实时方式流式输出每个标记的方法。这对于需要实现实时响应或逐步处理长文本的应用场景非常有用。 

7. **PEFT**

PEFT库的名称来源于Parameter-Efficient Fine-Tuning，指的是一种参数高效的微调方法。

这种方法可以使预训练模型适应各种下游应用程序，而无须微调模型的所有参数。

通过使用PEFT库，用户可以更有效地将预训练模型调整到特定任务上，同时减少计算和存储资源的消耗。 



#### 8.8.4 快速使用

下面展示一个使用Qwen1.5-0.5B-Chat模型进行多轮对话交互的示例。

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
device = "cuda" # the device to load the model onto
# 下载速度可能会很慢，耐心等待就可以
model = AutoModelForCausalLM.from_pretrained(
    "qwen/Qwen1.5-0.5B-Chat",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen1.5-0.5B-Chat")
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)
generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```
运行结果如图8-1所示。

![image](https://github.com/user-attachments/assets/33433e10-0187-4910-9bac-360a7dfb31cc)


若出现图8.1所示的结果，则说明Qwen1.5-0.5B-Chat模型已经部署成功。

#### 8.8.5 量化

本案例更新量化方案为基于AutoGPTQ的量化，提供Qwen1.5-0.5B-Chat的Int4量化模型。该方案在模型评测效果上几乎无损，且存储需求更低，推理速度更优。

下面通过示例来说明如何使用Int4量化模型。在开始使用之前，请先确保满足如下要求：torch库的版本为2.0及以上，transformers库的版本为4.37.0及以上等，并安装所需的安装包。

```bash

pip install auto-gptq optimum
```

如果在安装auto-gptq时遇到问题，则建议读者到repo官方网站上搜索合适的预编译wheel包。

接下来，可使用和8.8.4节一致的方法调用量化模型。

```python
model = AutoModelForCausalLM.from_pretrained(
    "qwen/Qwen1.5-0.5B-Chat",
    device_map="auto"
).eval()
response, history = model.chat(tokenizer, "你好", history=None)
```

### 8.9 基于LM Studio和AutoGen Studio使用通义千问
#### 8.9.1 LM Studio介绍
LM Studio是一个功能强大的跨平台桌面应用程序，旨在为用户提供一个简单明了的界面，以便在本地环境中运行和测试LLM。该应用程序使用户能够轻松地从Hugging Face平台下载并运行任何与GGML格式兼容的模型。LM Studio还配备了一系列简单且高效的工具，以帮助用户配置模型参数，并进行模型推理操作。对个人用户来说，LM Studio的应用尤其方便，因为它支持完全离线操作，即使在没有网络的情况下，用户也能在笔记本电脑上运行LLM。用户可以选择LM Studio内置的聊天界面或搭建一个与OpenAI兼容的本地服务器来运行和测试LLM。 

#### 8.9.2 AutoGen Studio介绍

AutoGen Studio是一个由微软公司开发的用户界面应用程序，建立在AutoGen框架之上，旨在促进多Agent工作流的快速设计，并可以展示终端用户界面。

AutoGen Studio的主要功能与特点如下。

- **Agent修改**：用户可以在AutoGen Studio界面上定义和修改Agent的参数，以及它们之间的通信方式。 

- **与Agent的互动**：通过直观的用户界面创建聊天会话，用户可以与指定的Agent进行交互。 

- **增加Agent技能**：用户可以显式地为他们的Agent增加技能，使其能够完成更多任务。例如，用户可以为Agent增加生成图片、获取网页正文或查找学术论文等技能。 

- **发布会话**：用户可以将他们的会话发布到本地画廊，以便与其他人分享或重用。 


#### 8.9.3 LM Studio的使用
1. **打开LM Studio**

打开LM Studio，其主界面如图8-2所示。

![image](https://github.com/user-attachments/assets/8f70f3ec-59d2-4cde-8ac9-988509718484)


2. **导入模型**

单击“My Models”图标打开模型文件夹，单击“导入模型”按钮将下载包中的qwen文件夹复制并粘贴到刚打开的模型文件夹中，需要导入的是GGUF格式的模型。 

![image](https://github.com/user-attachments/assets/74eb2ac3-07cb-4ae3-bd9c-a536bb349034)

![image](https://github.com/user-attachments/assets/e54dec9a-4a91-4dd4-81a2-92fe1b3487b5)



3. **使用模型**

在LM Studio界面（矩形框）中选择Qwen模型，即可启动会话。


![image](https://github.com/user-attachments/assets/f8187062-c41a-4604-b3f0-b88bc81ff455)


通过以上步骤，就可以愉快地使用LM Studio和Qwen1.5-0.5B模型进行本地LLM的运行和测试了。




#### 8.9.4 在LM Studio上启动模型的推理服务


![image](https://github.com/user-attachments/assets/512905c0-6bc4-4105-b56c-9add83ce8546)


单击“Start Server”按钮启动推理服务。

#### 8.9.5 启动AutoGen Studio服务
在命令提示符界面中输入如下命令，启动AutoGen Studio服务：
```bash
conda activate autogen
autogenstudio ui --port 8081
```


![image](https://github.com/user-attachments/assets/d70a516a-2758-4b78-9d09-a9dc11d959f7)


#### 8.9.6 进入AutoGen Studio界面


![image](https://github.com/user-attachments/assets/f2795683-c50d-4af8-b705-73cdc9533344)


打开浏览器，在浏览器地址栏中输入“http://127.0.0.1:8081/”，按回车键后进入AutoGen Studio界面。

#### 8.9.7 使用AutoGen Studio配置LLM服务

选择AutoGen Studio界面左侧的“Models”选项，并单击右侧的“+New Model”按钮，使用AutoGen Studio配置LLM。


![image](https://github.com/user-attachments/assets/7a389b71-0e8e-41e5-99da-d3a358ec7f30)


在对话框的相应输入框中输入如下信息：


```
Model Name: qwen/Qwen1.5-0.5B-Chat-GGUF
API Key: lm-studio
Base URL: http://localhost:1234/v1
```
详细信息可以到LM Studio模型启动界面中查看。 



![image](https://github.com/user-attachments/assets/5854b363-384b-4ae6-9501-e3c03b222fb0)
