第11章 Llama 3模型的微调、量化、部署和应用
sudo apt install git


sudo apt install cmake



![image](https://github.com/user-attachments/assets/cd8c9bf2-122c-463f-9ea9-26f19792342a)


```bash
startpro@startpro-virtual-machine:~$ sudo apt install git
[sudo] password for startpro:
Building package lists... Done
Reading state information... Done
All packages are up to date.
0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.
startpro@startpro-virtual-machine:~$ sudo apt install cmake
Building package lists... Done
Reading state information... Done
All packages are up to date.
0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.
startpro@startpro-virtual-machine:~$
```
图11-25

2. 克隆Llama.cpp仓库

先克隆程序，如图11-26所示。


![image](https://github.com/user-attachments/assets/002b26bc-2d54-4038-b03f-773622eb7739)


```bash
git clone https://***hub.com/ggerganov/llama.cpp
```
```bash
(base) root@startpro-virtual-machine:~# git clone https://***hub.com/ggerganov/llama.cpp
Cloning into 'llama.cpp'...
remote: Enumerating objects: 25647, done.
remote: Counting objects: 100% (9647/9647), done.
remote: Compressing objects: 100% (3567/3567), done.
remote: Total 25647 (delta 6259), reused 9496 (delta 6091), pack-reused 15995
Receiving objects: 100% (25647/25647), 47.28 MiB | 11.84 MiB/s, done.
Resolving deltas: 100% (16250/16250), done.
(base) root@startpro-virtual-machine:~#
```
图11-26


![image](https://github.com/user-attachments/assets/003b5e27-6ca2-4f3f-b900-72e74c7e3b4b)


然后创建一个新的Python虚拟环境，以便进行Llama.cpp项目的安装和使用，如图11-27所示。
```bash
cd llama.cpp
conda create -n llama_cpp python=3.10 -y
```
```bash
(base) root@startpro-virtual-machine:~# cd llama.cpp
(base) root@startpro-virtual-machine:~/llama.cpp# conda create -n llama_cpp python=3.10 -y
Collecting package metadata (repodata.json): done
Solving environment: done
## Package Plan ##
environment location: /opt/anaconda3/envs/llama_cpp
added / updated specs:
  - python=3.10
The following NEW packages will be INSTALLED:
  - libgcc_mutex-0.1-main
  - libstdcxx-ng-13.2.0-h046adac_2
  - python-3.10.13-h5840ade_0
  - _libgcc_mutex-0.1-conda_forge
  - _openmp_mutex-5.1-1_gnu
```
图11-27


![image](https://github.com/user-attachments/assets/12f24f82-b12a-4dfd-ac8b-3ce8a454e6a3)


激活环境，安装依赖库文件，如图11-28所示。
```bash
conda activate llama_cpp
pip install -r requirements/requirements-convert-hf-to-gguf.txt
```
```bash
(llama_cpp) root@startpro-virtual-machine:~/llama.cpp# conda activate llama_cpp
(llama_cpp) root@startpro-virtual-machine:~/llama.cpp# pip install -r requirements/requirements-convert-hf-to-gguf.txt
Collecting sympy>=1.11 (from -r requirements/requirements-convert.txt (line 1))
  Using cached sympy-1.12-py3-none-any.whl (5.0 MB)
Collecting torch==2.1.2+cpu (from -r requirements/requirements-convert.txt (line 2))
  Using cached torch-2.1.2%2Bcpu-cp310-cp310-manylinux2014_x86_64.whl.metadata (5.0 kB)
Collecting transformers==5.0.0 (from -r requirements/requirements-convert.txt (line 3))
  Using cached transformers-5.0.0-py3-none-any.whl.metadata (43 kB)
Collecting sentencepiece==0.1.99 (from -r requirements/requirements-convert.txt (line 4))
  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux2014_x86_64.whl.metadata (3.2 kB)
Collecting accelerate>=0.24.1 (from -r requirements/requirements-convert.txt (line 5))
  Using cached accelerate-0.24.1-py3-none-any.whl.metadata (541 bytes)
Collecting torch==2.1.1 (from -r requirements/requirements-convert.txt (line 2))
  Using cached torch-2.1.1%2Bcpu-cp310-cp310-manylinux2014_x86_64.whl.metadata (5.0 kB)
Collecting torch==2.1.0 (from -r requirements/requirements-convert.txt (line 3))
  Using cached torch-2.1.0%2Bcpu-cp310-cp310-manylinux2014_x86_64.whl.metadata (5.0 kB)
Collecting huggingface-hub>=0.14.0 (from -r requirements/requirements-convert.txt (line 3))
  Using cached huggingface_hub-0.14.0-py3-none-any.whl.metadata (12 kB)
Collecting tokenizers!=0.14.0,>=0.13.2 (from -r requirements/requirements-convert.txt (line 3))
  Using cached tokenizers-0.13.3-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.1 MB)
Collecting sympy>=1.5.1 (from transformers==5.0.0->-r requirements/requirements-convert.txt (line 3))
  Using cached sympy-1.5.1-py3-none-any.whl.metadata (2.1 MB)
```
图11-28


![image](https://github.com/user-attachments/assets/5fc80b43-2eb3-4769-8023-989db668bc5b)



配置和生成构建文件，如图11-29所示。
```bash
cmake -B build
```
```bash
(llama_cpp) root@startpro-virtual-machine:~/llama.cpp# cmake -B build
-- The C compiler identification is GNU 12.3.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1")
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- Warning: cache not found - consider installing it for faster compilation or disable this warning with LLAMA_CACHE_OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done
-- Generating done
-- Build files have been written to: /root/llama.cpp/build
```
图11-29

编译项目，如图11-30所示。

![image](https://github.com/user-attachments/assets/0a6e7208-3a2b-4f16-9aca-8245e4ae4ecd)


```bash
cmake --build build --config Release
```
```bash
(llama_cpp) root@startpro-virtual-machine:~/llama.cpp# cmake --build build --config Release
[  0%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_64x.c.o
[  5%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_32x.c.o
[ 10%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_16x.c.o
[ 15%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_8x.c.o
[ 20%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_4x.c.o
[ 25%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_2x.c.o
[ 30%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_1x.c.o
[ 35%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 40%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 45%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 50%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 55%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 60%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 65%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 70%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 75%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 80%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 85%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 90%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[ 95%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[100%] Building C object CMakeFiles/openblas.dir/src/openblas/axpy_kernel_0x.c.o
[100%] Linking C static library libopenblas.a
[100%] Built target openblas
[100%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[100%] Linking CXX static library libllama.a
[100%] Built target llama
[100%] Generating build_info.cmake
[100%] Built target build_info
```
图11-30


3. 模型转换

![image](https://github.com/user-attachments/assets/387bc798-22fe-4ca5-8f22-e2b12c887b44)



将指定路径下的模型文件转换为GGUF格式，并指定输出类型和输出文件路径，如图11-31和图11-32所示。
```bash
python -V
python convert-hf-to-gguf.py /opt/modes/2024-1 --outtype f16 --outfile /opt/2024-2-2-llama3-zh.gguf
```
```bash
(llama_cpp) root@startpro-virtual-machine:~/llama.cpp# python -V
Python 3.10.13
(llama_cpp) root@startpro-virtual-machine:~/llama.cpp# python convert-hf-to-gguf.py /opt/modes/2024-1 --outtype f16 --outfile /opt/2024-2-2-llama3-zh.gguf
llama.cpp: loading model from /opt/modes/2024-1 (in 4-bit Little Endian only)
llama_model_load: format: gguf v3 (latest)
llama_model_load: n_vocab      = 32000
llama_model_load: n_ctx        = 2048
llama_model_load: n_embd       = 4096
llama_model_load: n_mult       = 256
llama_model_load: n_head       = 32
llama_model_load: n_layer      = 32
llama_model_load: n_rot        = 128
llama_model_load: ftype        = 1 (mostly Q4_0)
llama_model_load: n_ff       = 11008
llama_model_load: model size   = 32000.00 MB
llama_model_load: gguf_ver     = 11
llama_model_load: model ftype  = 1 (mostly Q4_0)
llama_model_load: model params = 13B
llama_model_load: gguf_ver     = 11
llama_model_load: model ftype  = 1 (mostly Q4_0)
llama_model_load: model params = 13B
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load: kv self size = 122.00 MB
llama_model_load
```
### 4. 模型量化（q4_0）
应用q4_0量化方法，将模型权重从更高精度（如16位浮点数或32位浮点数）转换为更低精度的4位整数，并将量化后的模型保存到指定输出路径/opt/modes/2024-2-2-llama3-zh.gguf，如图11-33和图11-34所示。

![image](https://github.com/user-attachments/assets/7264baeb-5071-4087-bbb6-cec767a07662)


![image](https://github.com/user-attachments/assets/f66dc5f0-378e-4f4c-982b-e108e84e51a3)


```bash
./quantize /opt/2024-2-2-llama3-zh.gguf /opt/modes/2024-2-2-llama3-zh.gguf q4_0
```
```bash
(llama_cpp) root@startpro-virtual-machine:/opt/llama.cpp# ./quantize /opt/2024-2-2-llama3-zh.gguf /opt/modes/2024-2-2-llama3-zh.gguf q4_0
main: built with cc (Ubuntu 12.3.0-2ubuntu1~22.04) 12.3.0 for x86_64-pc-linux-gnu
llama_model_load: loading model from '/opt/2024-2-2-llama3-zh.gguf' as q4_0
llama_model_load: format: gguf v3 (latest)
llama_model_load: model size: 32000.00 MB
llama_model_load: gguf_ver: 11
llama_model_load: model ftype: 1 (mostly Q4_0)
llama_model_load: model params: 13B
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.00 MB
llama_model_load: kv self size: 122.0
```

### 11.4 模型部署
部署Llama 3模型是确保其在实际应用中能够高效运行的关键步骤。通过执行正确的部署流程，可以将模型从开发环境迁移到生产环境中，从而处理真实用户的请求。本节将详细介绍模型部署的相关内容，包括部署环境选择、部署流程详解。

#### 11.4.1 部署环境选择

在选择部署环境时，需要考虑应用需求、性能要求和成本等多个因素。常见的部署环境包括云端部署与本地部署。

1. **云端部署与本地部署**
    - **云端部署**：适用于需要具有高可用性、弹性扩展和全球访问的应用。云端部署可以利用云服务提供商（如AWS、Google Cloud、Azure、华为云、阿里云）提供的基础设施和服务，快速实现部署和扩展。
    - **本地部署**：适用于数据敏感、需要低延迟或无法连接互联网的应用。通过在本地服务器或私有数据中心进行部署，可以确保数据的安全和用户隐私。

2. **部署平台与框架选择**

选择合适的部署平台和框架可以简化部署过程并提高效率。常用的部署平台和框架如下。
    - **Docker**：一种容器化技术，允许将应用及其依赖打包在一个容器中，以确保在任何环境下都能一致运行。
    - **Kubernetes**：一个开源的容器编排系统，用于自动化部署、扩展和管理容器化应用。 
    - **TensorFlow Serving**：一个灵活的高性能服务系统，专为机器学习模型提供部署和推理服务。
    - **TorchServe**：一款用于部署PyTorch模型的工具，提供了多种服务功能，如模型管理、日志记录和监控。
    - **Ollama**：一个针对高性能模型进行部署和推理优化的框架，支持LLM的高效部署。

#### 11.4.2 部署流程详解
1. **下载Ollama**

下载Ollama模型，如图11-35所示。

![image](https://github.com/user-attachments/assets/b302ac96-0704-48f3-83c9-a9794be16447)


```bash
curl -fsSL https://***ama.com/install.sh | sh
```
```bash
(base) [22:30:01]default@iot ~#curl -fsSL https://***ama.com/install.sh | sh
curl: (7) Failed to connect to github.com port 443 after 21653 ms: Couldn't connect to server
(base) [22:31:02]default@iot ~#curl -fsSL https://***ama.com/install.sh | sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1075  100  1075    0     0   6249      0 --:--:-- --:--:-- --:--:--  6386
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1075  100  1075    0     0   6249      0 --:--:-- --:--:-- --:--:--  6386
Installing ollama to /usr/local/bin...
Adding ollama user to video group...
Adding ollama user to ollama group...
Creating ollama service...
Loading and starting ollama service...
ln -s '/opt/systemd/system/default.target.wants/ollama.service' to /etc/systemd/system/ollama.service.
Warning: The unit file, source configuration file or drop-ins of ollama.service changed on disk. Run 'systemctl daemon-reload' to reload units.
(base) [22:36:21]default@iot ~#
```
图11-35


2. **创建Modelfile文件**

创建Modelfile文件，如图11-36所示。

![image](https://github.com/user-attachments/assets/b017bfed-19a3-439f-a81c-e9adb57bde32)


```bash
cat > Modelfile
FROM /data/open-webui/models/2024-2-2-llama3-zh.gguf
EOF
```
```bash
(base) root@startpro-virtual-machine:/opt/ollama# cat Modelfile
FROM /data/open-webui/models/2024-2-2-llama3-zh.gguf
(base) root@startpro-virtual-machine:/opt/ollama#
```
图11-36

3. **创建模型**

创建模型，如图11-37所示。

![image](https://github.com/user-attachments/assets/66ff2473-1e41-4949-8ef1-830dd33553b2)


```bash
ollama create llama3-Chinese:8B -f Modelfile
```
```bash
(base) root@startpro-virtual-machine:/opt/ollama# ollama create llama3-Chinese:8B -f Modelfile
transferring model data
validating model
writing new layer sha256:f5e01947f8497c23954d5093af7a7f690b12e71f1a2301b7b002b2c815747d6
writing manifest
success
(base) root@startpro-virtual-machine:/opt/ollama#
```
图11-37

4. **运行模型**

运行模型，如图11-38所示。

![image](https://github.com/user-attachments/assets/cb25b0c5-dccb-4812-b06c-2d47cec11152)


```bash
(base) root@startpro-virtual-machine:/opt/ollama# ollama run llama3-Chinese:8B
端午节是中国传统节日之一，它有着悠久而丰富的历史和文化背景，据说这是为了纪念楚国诗人屈原。
端午节是一个充满活力的时刻，人们通常在这一天祭祀祖先，并庆祝农历五月初五的到来。它标志着夏季的开始，丰收的喜悦。
端午节中的文化活动非常多样化，有一些古老而又美丽的传统仪式，比如赛龙舟、吃粽子等。端午节是一个展示中华民族传统文化的绝佳机会。在这个特殊时刻，我们可以通过欣赏赛龙舟、品尝粽子等方式，了解和体验中华文化中的智慧与创新。此外，端午节在中国有着深厚的历史底蕴，它代表了民族团结、爱国主义以及传统价值观念。它不仅是我们中华民族的一个重要文化符号，更体现出了我们坚韧不拔、勇敢的品质。
在这个美好的节日里，我们应该充满热情，感受着中华民族的厚重底蕴，将端午节当作一个展示中国传统文化的窗口。让我们一起欢庆这一天，祝大家端午节快乐！祝愿祖国繁荣昌盛！祝愿中华民族团结互助、共同发展！
^Cand a message (?) for help)
```
图11-38

### 11.5 低代码应用示例

本节使用Ollama+WebUI+AnythingLLM，构建安全可靠的个人/企业知识库。

#### 11.5.1 搭建本地大语言模型

11.4节已经在本地完成了Llama 3模型部署，本节直接使用已部署好的模型。

如果读者使用Windows平台进行部署，则可以前往Ollama官方网站下载Windows操作系统的安装包，如图11-39所示。下载完成后，直接安装即可。

拉取大语言模型。打开终端，输入如下代码，即可自动下载Llama 3模型，如图11-40所示。


![image](https://github.com/user-attachments/assets/67f85b7f-5a0c-408a-91d4-c12aef5b627a)


```bash

ollama run llama3
```
```bash
PS C:\Users\user> ollama run llama3
pulling llama3:latest
100.00% [==========================================================] 0/0
pulling 47.64MB/47.64MB [==========================================================] 4.76 MB/s
pulling 94.84MB/94.84MB [==========================================================] 11.85 MB/s
pulling 110.00MB/110.00MB [==========================================================] 11.00 MB/s
pulling 125.20MB/125.20MB [==========================================================] 10.00 MB/s
writing docker layers
removing unused layers
2024-07-01 15:22:23 INFO:    Loading model...
2024-07-01 15:22:23 INFO:    - type: llama
2024-07-01 15:22:23 INFO:    - version: 2.2.0
2024-07-01 15:22:23 INFO:    - build: 8625082c
2024-07-01 15:22:23 INFO:    - size: 7B
2024-07-01 15:22:23 INFO:    - quant: none
2024-07-01 15:22:23 INFO:    - module: main
2024-07-01 15:22:23 INFO:    - offload: 0
2024-07-01 15:22:23 INFO:    - gpu: 0
2024-07-01 15:22:23 INFO:    - seed: 1337
2024-07-01 15:22:23 INFO:    - prompt template: llama-2
2024-07-01 15:22:23 INFO:    - system prompt: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.
If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
2024-07-01 15:22:23 INFO:    - language: en
2024-07-01 15:22:23 INFO:    - multi-line: false
2024-07-01 15:22:23 INFO:    - interactive: false
2024-07-01 15:22:23 INFO:    - interactive-start: false
2024-07-01 15:22:23 INFO:    - top_k: 40
2024-07-01 15:22:23 INFO:    - top_p: 0.9
2024-07-01 15:22:23 INFO:    - temperature: 0.6
2024-07-01 15:22:23 INFO:    - repeat_penalty: 1.1
2024-07-01 15:22:23 INFO:    - max_tokens: 2048
2024-07-01 15:22:23 INFO:    - stop: ["<|endoftext|>"]
2024-07-01 15:22:23 INFO:    - debug: false
2024-07-01 15:22:23 INFO:    - color: true
2024-07-01 15:22:23 INFO:    - echo: false
2024-07-01 15:22:23 INFO:    - frequency_penalty: 0.0
2024-07-01 15:22:23 INFO:    - presence_penalty: 0.0
2024-07-01 15:22:23 INFO:    - mirostat: 0
2024-07-01 15:22:23 INFO:    - mirostat_lr: 0.1
2024-07-01 15:22:23 INFO:    - mirostat_ent: 5.0
2024-07-01 15:22:23 INFO:    - model: C:\Users\user\.ollama\llama3\latest\model
2024-07-01 15:22:23 INFO:    - embedding: false
2024-07-01 15:22:23 INFO:    - embedding-model: C:\Users\user\.ollama\llama3\latest\model
2024-07-01 15:22:23 INFO:    - cache: C:\Users\user\.cache\ollama\llama3
2024-07-01 15:22:23 INFO:    - context: 2048
2024-07-01 15:22:23 INFO:    - threads: 8
2024-07-01 15:22:23 INFO:    - n_batch: 512
2024-07-01 15:22:23 INFO:    - n_gpu_layers: 0
2024-07-01 15:22:23 INFO:    - last_n_tokens: 64
2024-07-01 15:22:23 INFO:    - use_mmap: true
2024-07-01 15:22:23 INFO:    - use_mlock: false
2024-07-01 15:22:23 INFO:    - use_mlock2: false
2024-07-01 15:22:23 INFO:    - use_jemalloc: false
2024-07-01 15:22:23 INFO:    - use_tcmalloc: false
2024-07-01 15:22:23 INFO:    - use_hugepages: false
2024-07-01 15:22:23 INFO:    - use_numa: false
2024-07-01 15:22:23 INFO:    - use_fma: true
2024-07-01 15:22:23 INFO:    - use_avx2: true
2024-07-01 15:22:23 INFO:    - use_avx512: false
2024-07-01 15:22:23 INFO:    - use_f16c: true
2024-07-01 15:22:23 INFO:    - use_f16c_vmem: false
2024-07-01 15:22:23 INFO:    - use_amx: false
2024-07-01 15:22:23 INFO:    - use_bfloat16: false
2024-07-01 15:22:23 INFO:    - use_ggml_fp16: false
2024-07-01 15:22:23 INFO:    - use_ggml_fp8: false
2024-07-01 15:22:23 INFO:    - use_ggml_q4_0: false
2024-07-01 15:22:23 INFO:    - use_ggml_q4_1: false
2024-07-01 15:22:23 INFO:    - use_ggml_q5_0: false
2024-07-01 15:22:23 INFO:    - use_ggml_q5_1: false
2024-07-01 15:22:23 INFO:    - use_ggml_q8_0: false
2024-07-01 15:22:23 INFO:    - use_ggml_q2_K: false
2024-07-01 15:22:23 INFO:    - use_ggml_q3_K: false
2024-07-01 15:22:23 INFO:    - use_ggml_q6_K: false
2024-07-01 15:22:23 INFO:    - use_ggml_q4_K: false
2024-07-01 15:22:23 INFO:    - use_ggml_q5_K: false
2024-07-01 15:22:23 INFO:    - use_ggml_q8_K: false
2024-07-01 15:22:23 INFO:    - use_ggml_q2_K: false
2024-07-01 15:22:23 INFO:    - use_ggml_q3_K: false
2024-07-01 15:22:23 INFO:    - use_ggml_q6_K: false
2024-
```

![image](https://github.com/user-attachments/assets/7f4f6755-09e6-4fb4-a33a-467cb44b0380)




![image](https://github.com/user-attachments/assets/d70e8ed1-52f5-4f78-914b-e7fb461ba100)



![image](https://github.com/user-attachments/assets/d498e7a4-cf3e-4769-805f-0659fad7ea66)


![image](https://github.com/user-attachments/assets/83930405-0a88-43a9-bb4f-e0926257e4c0)

![image](https://github.com/user-attachments/assets/7a397dfe-751c-4976-bc59-abbb4c013bcd)


![image](https://github.com/user-attachments/assets/b727929b-7f55-4e10-9cc5-fd087a33da1f)






