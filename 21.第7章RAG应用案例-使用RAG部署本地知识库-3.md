### AI Agent应用与项目实战


![image](https://github.com/user-attachments/assets/c43e1f79-769b-41ad-ad06-1845cdbec472)


图7-12

```js
INFO Loading knowledge base
DEBUG Creating collection
DEBUG Checking if table exists: local_rag_documents_nomic_embed_text
DEBUG Upserted document: https://www.***paper.cn/newsDetail_forward_26883985_1 |
https://www.***paper.cn/newsDetail_forward_26883985 | {'url': 'https://www.***paper.cn/newsDetail_forward_26883985', 'chunk': 1, 'chunk_size': 2996}
DEBUG Upserted document: https://www.***paper.cn/newsDetail_forward_26883985_2 |
https://www.***paper.cn/newsDetail_forward_26883985 | {'url': 'https://www.***paper.cn/newsDetail_forward_26883985', 'chunk': 2, 'chunk_size': 279}
INFO Committed 2 documents
INFO Loaded 2 documents to knowledge base
```

从图7-12中可以看到，程序调用了本地的nomic - embed - text模型，并且对上传的网络链接内容进行了向量化拆分，表示上传的网络链接读取成功。

#### 4. 前端页面展示（4）
这里上传了PDF文档（见图7-13）。

![image](https://github.com/user-attachments/assets/5885e48b-094e-4bd2-a834-247489771bcb)



图7-13

Add a PDF 

Drag and drop file here

Limit 200MB per file - PDF

Browse files



相关对应代码如下：

```python

# 添加PDF文档并上传到知识库中
if "file_uploader_key" not in st.session_state:
    st.session_state["file_uploader_key"] = 100
uploaded_file = st.sidebar.file_uploader("Add a PDF :page_facing_up:", type="pdf", key=st.session_state["file_uploader_key"])
if uploaded_file is not None:
    alert = st.sidebar.info("Processing PDF...")
    rag_name = uploaded_file.name.split(".")[0]
    if f"{rag_name}_uploaded" not in st.session_state:
        reader = PDFReader()
        rag_documents: List[Document] = reader.read(uploaded_file)
        if rag_documents:
            rag_assistant.knowledge_base.load_documents(rag_documents, upsert=True)
        else:
            st.sidebar.error("Could not read PDF")
    st.session_state[f"{rag_name}_uploaded"] = True
    alert.empty()
```
**代码说明**：

（1）uploaded_file = st.sidebar.file_uploader("Add a PDF :page_facing_up:", type="pdf", key=st.session_state["file_uploader_key"])表示单击“Browse files”按钮上传PDF文档，会把相关信息赋值给uploaded_file变量。

（2）通过if语句将PDF文档上传到知识库中。

（3）单击“Browse files”按钮且上传完PDF文档后，后台显示上传的PDF文档被拆分为15块文档数据，如图7-14所示。

图7-14
```js
DEBUG Checking if table exists: local_rag_documents_nomic_embed_text
DEBUG Upserted document: James H_1_1 | James H | {'page': 1, 'chunk': 1, 'chunk_size': 312}
DEBUG Upserted document: James H_2_1 | James H | {'page': 2, 'chunk': 1, 'chunk_size': 3080}
DEBUG Upserted document: James H_2_2 | James H | {'page': 2, 'chunk': 2, 'chunk_size': 1807}
DEBUG Upserted document: James H_3_1 | James H | {'page': 3, 'chunk': 1, 'chunk_size': 2996}
DEBUG Upserted document: James H_3_2 | James H | {'page': 3, 'chunk': 2, 'chunk_size': 2648}
DEBUG Upserted document: James H_4_1 | James H | {'page': 4, 'chunk': 1, 'chunk_size': 2994}
DEBUG Upserted document: James H_4_2 | James H | {'page': 4, 'chunk': 2, 'chunk_size': 1738}
DEBUG Upserted document: James H_5_1 | James H | {'page': 5, 'chunk': 1, 'chunk_size': 2994}
DEBUG Upserted document: James H_5_2 | James H | {'page': 5, 'chunk': 2, 'chunk_size': 2834}
DEBUG Upserted document: James H_6_1 | James H | {'page': 6, 'chunk': 1, 'chunk_size': 3000}
DEBUG Upserted document: James H_6_2 | James H | {'page': 6, 'chunk': 2, 'chunk_size': 2336}
DEBUG Upserted document: James H_7_1 | James H | {'page': 7, 'chunk': 1, 'chunk_size': 2999}
DEBUG Upserted document: James H_7_2 | James H | {'page': 7, 'chunk': 2, 'chunk_size': 2999}
DEBUG Upserted document: James H_7_3 | James H | {'page': 7, 'chunk': 3, 'chunk_size': 332}
DEBUG Upserted document: James H_8_1 | James H | {'page': 8, 'chunk': 1, 'chunk_size': 704}
INFO Committed 15 documents
INFO Loaded 15 documents to knowledge base
```


![image](https://github.com/user-attachments/assets/50e391a0-9f2e-453b-b382-c39041d3aac0)


从图7-14中可以看到，程序调用了本地的nomic - embed - text模型，并且对上传的PDF文档内容进行了向量化拆分，表示上传的PDF文档读取成功。

#### 5. 前端页面展示（5）


![image](https://github.com/user-attachments/assets/1b54ef37-33a0-4c13-a5aa-43a534fc868f)


清除已上传的PDF文档及网络链接的知识库（见图7-15）。

清除成功后输入框中将显示“Knowledge base cleared”（见图7-16）。


图7-15

Add a PDF 

Drag and drop file here

Limit 200MB per file - PDF

Browse files

Clear Knowledge Base



图7-16

Add a PDF 

Drag and drop file here

Limit 200MB per file - PDF

Browse files

James H. Simons, PhD...

111.pdf

Clear Knowledge Base

Knowledge base cleared



```python

if rag_assistant.knowledge_base and rag_assistant.knowledge_base.vector_db:

    if st.sidebar.button("Clear Knowledge Base"):
        rag_assistant.knowledge_base.vector_db.clear()
        st.sidebar.success("Knowledge base cleared")
```

**代码说明**：

（1）st.sidebar.button函数用于设置按钮。

（2）rag_assistant.knowledge_base.vector_db.clear用于清除知识库。

（3）st.sidebar.success用于设置单击按钮清除成功后的提示。


#### 6. 与Llama 3交互展示
到目前为止，如果上述步骤都没有问题，则RAG应用本地部署就完成了。现在来看一下效果，前端交互页面如图7-17所示。


![image](https://github.com/user-attachments/assets/a2fac07e-a519-46a7-8f4f-5c346964bb44)




图7-18所示为笔者和本地部署的Llama 3模型的交互过程。


![image](https://github.com/user-attachments/assets/7aeed8d0-67f1-4290-b09e-d7451a282e54)



图7-18

问：When you create your investment models, do the models explicitly take into account things like the monetary environment, including what the Fed is doing? Or do they simply rely on statistical analysis?

答：We Explicitly Incorporate Macro - Financial Factors

When developing our investment models, we deliberately consider a range of macro - financial factors that influence market behavior. This includes the monetary environment, such as the actions and decisions made by central banks like the Federal Reserve (Fed). Our models are designed to incorporate these external factors to provide more accurate and robust predictions.

Understanding the Interplay between Markets

By considering the Fed's policy decisions and their impact on interest rates, inflation, employment, and other key economic indicators, we can better anticipate how these factors will influence asset prices. This allows us to refine our models and make more informed investment decisions that take into account the complex interplay between markets.

Beyond Statistical Analysis

While statistical analysis is essential for building reliable models, it's not enough on its own. Our approach combines both quantitative and qualitative elements to create a comprehensive framework that accounts for the intricate relationships between various market drivers. This enables us to generate more accurate predictions and make more informed investment decisions in response to changing monetary conditions.


In summary, our investment models explicitly consider macro - financial factors like the Fed's actions to provide a more nuanced understanding of market behavior and inform our investment strategies.



**翻译**：

笔者的问题：当你在创建投资模型时，这些模型是否明确考虑了货币环境等因素，包括美联储的行动？还是仅仅依赖于统计分析？

Llama 3的回答：

我们会明确将宏观金融因素纳入考量

在开发我们的投资模型时，我们有意识地考虑了一系列影响市场行为的宏观金融因素。这包括货币环境，例如美联储（Fed）等中央银行的行动和决策。我们的模型旨在纳入这些外部因素，以提供更准确和稳健的预测。

理解市场之间的相互作用

通过考虑美联储的政策决策及其对利率、通货膨胀、就业和其他关键经济指标的影响，我们可以更好地预测这些因素将如何影响资产价格。这使我们能够改进我们的模型并在考虑市场之间复杂的相互作用的情况下做出更加明智的投资决策。

超越统计分析

虽然统计分析对于构建可靠的模型至关重要，但它本身是不够的。我们的方法结合了定量和定性要素，创建了一个全面的框架，考虑到了各种市场驱动因素之间复杂的关系。这使我们能够生成更准确的预测，并在不断变化的货币条件下做出更明智的投资决策。

综上所述，我们的投资模型明确考虑了像美联储行动这样的宏观金融因素，以提供对市场行为更细致的理解，并指导我们的投资策略。



通过后台，我们可以看到大语言模型本次回答的效率指标（见图7-19）。


图7-19

ASSISTANT:

DEBUG Time to first token: 83.98265s

DEBUG Tokens generated: 242

DEBUG Time per output token: 0.5578s

DEBUG Throughput: 1.7927 tokens/s

DEBUG Time to generate response: 134.9934s

![image](https://github.com/user-attachments/assets/2e6f1f16-29fb-4e69-8596-38b21f959d03)



其中，回答用时超过了2分钟，这跟本地的算力，尤其是图形处理器（Graphics Processing Unit，GPU）有关。

现在让大语言模型Llama 3读取一个采访James Simons（量化交易鼻祖，文艺复兴科技公司的创始人）的PDF文档，并把RUN ID设置为跟上一个问题一致，随后问Llama 3同样的问题，其回答如下（见图7-20）。

**翻译**：

笔者的问题：当你在创建投资模型时，这些模型是否明确考虑了货币环境等因素，包括美联储的行动？还是仅仅依赖于统计分析？

Llama 3的回答：据我所知，由James Simons和他的团队在文艺复兴科技公司开发的Medallion基金的投资模型主要侧重于数学和统计分析。据James Simons所说，他们的方法就是“坚持数学”，依赖这些工具的预测，而不是考虑其他流行的趋势或偏见。

虽然这些模型确实考虑了市场状况和宏观经济因素，但它们并没有明确纳入美联储（Fed）的货币政策决策信息。其重点更多的是使用先进的数学和统计技术来识别金融市场中的模式和异常。

![image](https://github.com/user-attachments/assets/4845d254-8e71-4cd2-a5ac-d7730f0ce03b)


图7-20
问：When you create your investment models, do the models explicitly take into account things like the monetary environment, including what the Fed is doing? Or do they simply rely on statistical analysis?

答：RAGit's Response

Based on my knowledge, Medallion's investment models, developed by James Simons and his team at Renaissance Technologies, primarily focus on mathematical and statistical analysis. According to James Simons, their approach is to "stick to the math" and rely on predictions from these tools rather than considering other popular trends or biases.

While the models do take into account market conditions and macroeconomic factors, they do not explicitly incorporate information about monetary policy decisions made by the Federal Reserve (Fed). The focus is more on identifying patterns and anomalies in financial markets using advanced mathematical and statistical techniques.

In other words, Medallion's investment models are built primarily around statistical analysis and machine learning algorithms, rather than relying on explicit consideration of macroeconomic or monetary policy factors.


通过查看文档内容可以发现，Llama 3后面的回答和上传的原文的意思是相符的。


通过后台，我们可以看到大语言模型本次回答的效率指标（见图7-21）。

![image](https://github.com/user-attachments/assets/d800d3ad-0339-4432-a2b7-9fba18d3290a)



图7-21

ASSISTANT:

DEBUG Time to first token: 103.6358s

DEBUG Tokens generated: 158

DEBUG Time per output token: 0.9018s

DEBUG Throughput: 1.1089 tokens/s

DEBUG Time to generate response: 142.4885s


### 7.3 调用云端大语言模型

7.1节和7.2节介绍了如何把大语言模型部署到本地实现RAG应用，但大语言模型本地运行对计算机配置及算力的要求是很高的，尤其是显卡，如果计算机配置不高，则用户体验感会很差，还会占用计算机的大量算力资源。

我们可以尝试调用云端大语言模型的算力来解决这一问题，目前，Groq平台提供了较好的解决方案。



Groq是为机器学习和其他高性能计算任务设计的一个硬件和软件平台。Groq的核心是一种高度并行的处理单元，被称为TensorFlow处理器。



以下是Groq的一些关键特性和概念。
- **硬件特性**：
  - **处理器架构**：Groq开发了一种名为Tensor Streaming Matrix（TSM）Unit的处理器架构，它是为处理深度学习和其他并行计算任务而设计的。
  - **高度并行性**：Groq的处理器架构用于同时执行大量操作，这对机器学习和其他需要执行大量并行计算的应用非常有用。
  - **可扩展性**：Groq的架构允许通过增加处理器数量来扩展性能，而不需要改变软件代码。
  - **灵活性**：Groq的处理器不仅适用于机器学习任务，还可以用于其他需要执行高性能计算的场景。
- **软件特性**：
  - **TensorFlow兼容性**：Groq的处理器专为TensorFlow设计，可以高效执行TensorFlow操作。
  - **易于编程**：Groq提供了易于使用的软件工具，使得开发者可以快速将他们的TensorFlow模型部署到Groq硬件上。
- **应用场景**：
  - **机器学习训练和推理**：Groq的处理器非常适合训练和运行复杂的机器学习模型。
  - **图像和视频处理**：Groq凭借其高并行性可以快速处理大量图像和视频数据。
  - **科学计算和数据分析**：Groq的处理器可以加速科学计算和数据分析任务的执行，特别是那些需要处理大规模数据集的任务。
- **优势**：
  - **性能**：Groq的处理器性能出色，在处理复杂的机器学习模型时非常有优势。
  - **效率**：Groq采用高度并行的设计，其处理器在执行机器学习任务时能效比较高。
  - **易用性**：Groq提供了简化的开发工具和库，使得开发者可以很容易地利用其硬件优势。
- **社区和生态系统**：
  - Groq拥有一个活跃的开发者社区，通过提供支持和资源来帮助开发者利用其技术。
  - 随着机器学习和AI的不断发展，Groq也在不断扩展其生态系统，包括软件库、工具和合作伙伴。

Groq是一个不断发展的平台，其特性和功能可能会随着时间而变化。如果读者需要获取其最新的信息，则建议访问Groq官方网站或联系其技术支持团队。

#### 7.3.1 配置大语言模型的API Key
本节介绍如何获取启动大语言模型的云端钥匙，流程如下。


1. **GROQ_API_KEY**：从Groq官方网站注册并获取创建API Key，如图7-22所示。

![image](https://github.com/user-attachments/assets/348f99eb-c702-4fc2-802c-b6344a8be80c)


图7-22

groqcloud

Playground

Documentation

API Keys

Settings

API Keys

Manage your API keys. Remember to keep your API keys safe to prevent unauthorized access.

NAME  SECRET KEY  CREATED  LAST USED

groqrag  gsk_...6df1  2024/6/17  2024/6/19

llama3  gsk_...103u  2024/6/17  2024/6/17

Create API Key


![image](https://github.com/user-attachments/assets/ee2b4864-ec7a-45ed-9c6d-c74d72b7edef)


在图7-22所示界面的“NAME”栏中输入名称后，就配置好了自己的API Key。需要注意的是，这里的API Key只会显示一次，要立刻保存起来。

2. **设置大语言模型的环境变量**

**配置GROQ_API_KEY**：

（1）在Windows“开始”菜单中选择“设置”命令，在上方“查找设置”搜索框中输入“环境变量”。

（2）选择“搜索结果”界面中的“编辑系统环境变量”选项。


（3）在弹出的“系统属性”对话框中单击“高级”选项卡中的“环境变量”按钮。

（4）单击“新建”按钮，在弹出页面的“变量”栏中输入“GROQ_API_KEY”，在“值”栏中输入API Key，如图7-23所示。


![image](https://github.com/user-attachments/assets/1f519d3b-a244-4624-b352-9a2ba808c9e7)



图7-23

变量 值

GITHUB_SERVER_URL 

GITHUB_TOKEN 

GROQ_API_KEY  gsk_ahBZKyzbFLGJ4tz3R8GWdyb3FWCScicKqwYmGwmQutL...


3. **设置embedding模型的环境变量**


![image](https://github.com/user-attachments/assets/3ac4a51e-717f-475d-ac27-46d3fc56a7ce)


到目前为止，Groq还不支持embedding模型，所以需要用到OpenAI的text - embedding - 3 - large模型。
首先从OpenAI官方网站获取OPEN_API_KEY，然后


设置embedding模型的环境变量。

**配置OPENAI_API_KEY**：

（1）在Windows“开始”菜单中选择“设置”命令，在上方“查找设置”搜索框中输入“环境变量”。

（2）选择“搜索结果”界面中的“编辑系统环境变量”选项。 

（3）在弹出的“系统属性”对话框中单击“高级”选项卡中的“环境变量”按钮。 

（4）单击“新建”按钮，在弹出页面的“变量”栏中输入“OPENAI_API_KEY”，在“值”栏中输入API Key，如图7 - 24所示。



图7 - 24

变量 值


GITHUB_SERVER_URL 

GITHUB_TOKEN 

GROQ_API_KEY  gsk_ahBZKyzbFLGJ4tz3R8GWdyb3FWCScicKqwYmGwmQutL...

OPENAI_API_KEY  sk - pKc7ZEmMabMbpQzWm1st3B3bMF7FNh09eYFZvOUp...


#### 7.3.2 修改本地RAG应用代码

本节对7.2节中部署本地RAG应用的代码进行修改。

1. **修改app.py的代码**

（1）在主程序函数main的获取大语言模型的模块中，添加可选大语言模型“llama3 - 70b - 8192”，如图7 - 25所示。


![image](https://github.com/user-attachments/assets/52c1aae6-7618-4621-a52d-6ee6540b611f)


图7 - 25
```python
def main() -> None:
    # 获取大语言模型，options=["Llama 3", "openhermes", "Llama 2"]为模型名称的可选项
    llm_model = st.sidebar.selectbox("Select Model", options=["Llama 3", "openhermes", "Llama 2", "llama3-70b-8192"])
    if "llm_model" not in st.session_state:
        st.session_state["llm_model"] = llm_model
    # 如果大语言模型选择发生变化，则重启RAG助手
    elif st.session_state["llm_model"] != llm_model:
        st.session_state["llm_model"] = llm_model
        restart_assistant()
```

（2）在主程序函数main的获取embedding模型的模块中，添加可选embedding模型“text - embedding - 3 - large”，如图7 - 26所示。


![image](https://github.com/user-attachments/assets/86b360bf-ef92-4d09-9a3f-540e0d5fae15)


图7 - 26
```python
# 获取embedding模型
embeddings_model = st.sidebar.selectbox(
    "Select Embeddings",options=["nomic-embed-text", "shunyue/llama3-chinese-shunyue", "phi3", "text-embedding-3-large"],
    help="When you change the embeddings model, the documents will need to be added again."
)
if "embeddings_model" not in st.session_state:
    st.session_state["embeddings_model"] = embeddings_model
# 如果embedding模型选择发生变化，则重启RAG助手
elif st.session_state["embeddings_model"] != embeddings_model:
    st.session_state["embeddings_model"] = embeddings_model
    st.session_state["embeddings_model_updated"] = True
    restart_assistant()
```

2. **修改assistant.py的代码**


（1）在原代码的基础上导入两个库，如图7 - 27所示。


![image](https://github.com/user-attachments/assets/e70cca09-e5bc-4814-a7cb-94fbaa82f8ea)


图7 - 27
```python
from typing import Optional
from phi.assistant import Assistant
from phi.knowledge import AssistantKnowledge
from phi.llm.ollama import Ollama
from phi.embedder.ollama import OllamaEmbedder
from phi.vectordb.pgvector import PGvector
from phi.storage.assistant.postgres import PgAssistantStorage
from phi.embedder.openai import OpenAIEmbedder
from phi.llm.groq import Groq
```

（2）加入OpenAI嵌入文本处理语句，如图7 - 28所示。

（此处文档未完整展示图7 - 28相关代码内容 ） 
