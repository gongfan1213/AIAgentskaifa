### 图片一
| AI Agent 应用与项目实战

```js
def get_rag_assistant(
    llm_model: str = "llama3",
    embeddings_model: str = "nomic-embed-text",
    user_id: Optional[str] = None,
    run_id: Optional[str] = None,
    debug_mode: bool = True
) -> Assistant:
    """Get a Local RAG Assistant."""

    # 基于embedding模型定义嵌入器
    embedder = OllamaEmbedder(model=embeddings_model, dimensions=4096)
    embeddings_model_clean = embeddings_model.replace("-", "_")
    if embeddings_model == "nomic-embed-text":
        embedder = OllamaEmbedder(model=embeddings_model, dimensions=768)
    elif embeddings_model == "phi3":
        embedder = OllamaEmbedder(model=embeddings_model, dimensions=3872)
    elif embeddings_model == "shunyu/llama3-chinese-shunyu":
        embedder = OllamaEmbedder(model=embeddings_model, dimensions=768)
    else:
        embedder = OpenAIEmbedder(model=embeddings_model, dimensions=1536)
```

![image](https://github.com/user-attachments/assets/81c05bd8-ee69-4d37-9a55-d2ed07d44c6a)



图7-28

（3）把7.2.1节的“3.get_rag_assistant函数”的“llm=Ollama(model=llm_model)”中的

“Ollama”修改为“Groq”，如图7-29所示。
```
llm=Groq(model=llm_model)
return Assistant(
    name="local_rag_assistant",
    run_id=run_id,
    user_id=user_id,
    llm=Groq(model=llm_model)
)
```


![image](https://github.com/user-attachments/assets/cfc79387-580e-494d-968c-40fab4dc6b6c)


图7-29

### 7.3.3 启动并调用云端大语言模型

按照7.2.3节的运行方法，使用“streamlit run”命令运行新的主程序app.py。

这次我们选用的大语言模型为“llama3-70b-8192”，embedding模型为“text-embedding-3-large”，前端交互页面如图7-30所示。

效果展示：

此处提出的问题与7.2.4节本地部署大语言模型的一样，第一次提问与回答如图7-31


![image](https://github.com/user-attachments/assets/dac3c631-4a30-441d-a4b6-07dd35ad2bc8)


![image](https://github.com/user-attachments/assets/e11d31f2-09cd-4594-b0ab-fe14d41e9300)

### 图片二

### 第7章 RAG应用案例——使用RAG部署本地知识库

迪哥的Agent之Local Rag

这个案例用于演示Llama 3应用与向量化检索



Upload a doc and ask me questions...

图7-30

迪哥的Agent之Local Rag

这个案例用于演示Llama 3应用与向量化检索


Upload a doc and ask me questions...

When you create your investment models, do the models explicitly take into account things like the monetary environment, including what the Fed is doing? Or do they simply rely on statistical analysis?

Investment Models and Monetary Environment

Our investment models take into account the monetary environment, including the Federal Reserve's actions, in addition to relying on statistical analysis. This multifaceted approach allows us to create more comprehensive and informed models.

图7-31

### 图片三
| AI Agent应用与项目实战

翻译：



用户的提问：当你在创建投资模型时，这些模型是否明确考虑了货币环境等因素，包括美联储的行动？还是仅仅依赖于统计分析？

llama3-70b-8192的回答：投资模型与货币环境

我们的投资模型在依赖统计分析的基础上，还考虑了货币环境，包括美联储的行动。这种多方面的方法使我们能够创建更全面、更明智的模型。

通过后台，我们可以看到大语言模型本次回答的时间（见图7-32）。



![image](https://github.com/user-attachments/assets/31ecde2e-f2c9-4ab4-92d3-4c9d51d1db50)


ASSISTANT:

DEBUG Time to generate response: 7.3672s



图7-32


这和本地部署大语言模型回答的时间相比，速度大幅提升。

现在让大语言模型llama3-70b-8192读取一个采访James Simons（量化交易鼻祖，文艺复兴科技公司的创始人）的PDF档，并把RUN ID设置为跟上一个问题一致，随后问llama3-70b-8192同样的问题，其回答如下（见图7-33）。


When you create your investment models, do the models explicitly take into account things like the monetary environment, including what the Fed is doing? Or do they simply rely on statistical analysis?

Statistical Analysis Takes Center Stage

According to James Simons, the founder of Renaissance Technologies, their investment models rely heavily on statistical analysis and mathematical approaches to investing. While they do incorporate new information and datasets as they become available, there is no explicit consideration of the monetary environment or the Fed's actions in their models. Instead, they focus on identifying patterns and signals in the data to inform their investment decisions.


![image](https://github.com/user-attachments/assets/56d33d66-663f-4373-9023-faa9af51c4d7)


图7-33

翻译：

统计分析占据中心舞台

据文艺复兴科技公司的创始人James Simons所说，他们的投资模型在很大程度上依赖于统计分析和数学方法来进行投资。虽然他们会在新的信息和数据集可用时纳入这些信息，但他们的模型并没有明确考虑货币环境或美联储的行动。相反，他们专注于在数据中



### 图片四

### 第7章 RAG应用案例——使用RAG部署本地知识库

识别模式和信号，以指导他们的投资决策。

可以看出，上传PDF文档后，答案有所不同，笔者认为该回答比本地大语言模型的回答更简练精准。

通过后台，我们可以看到大语言模型本次回答的时间（见图7-34）。


![image](https://github.com/user-attachments/assets/d000bd35-64f5-41ed-ba39-01358cb0d4db)


DEBUG Time to generate response: 2.272s

DEBUG ************** assistant **************

图7-34

比上次快多了，这与当时的网络条件有关。

本章通过详细的步骤和示例展示了如何在本地部署一个基于RAG技术的知识库系统。整个流程包括创建虚拟环境、安装所需库和工具、下载和管理大语言模型及embedding模型、配置向量数据库，并通过streamlit库实现交互式前端页面，最终构建出一个可以结合上下文进行智能回答的RAG助手。

另外，根据实际情况，如果本地计算机配置不满足要求或者有速度需求，则可以选择通过API Key调用云端大语言模型来实现。 

