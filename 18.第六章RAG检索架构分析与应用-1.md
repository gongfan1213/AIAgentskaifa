### 第6章 RAG检索架构分析与应用
以ChatGPT为代表的LLM的问世，标志着AIGC（Artificial Intelligence Generated Content，生成式人工智能）进入了新的快速发展阶段，其对学术界和工业界产生了深远影响。LLM通过对海量数据的深入学习，成为理解与应用自然语言的尖端工具，展示了强大的能力。然而，随着应用的普及，LLM也暴露出了一些关键性的问题，尤其是其对庞大数据集的依赖。这种依赖限制了LLM在完成训练后接纳新信息的能力，带来了三大挑战：①为了追求广泛的适用性和易用性，LLM在专业领域中的表现可能不尽如人意；②网络数据增长速度快，数据标注和模型训练需要耗费大量资源和算力，使得LLM难以持续快速更新；③LLM有时会生成令人信服但实际上不准确的答案，即产生所谓的LLM“幻觉”，可能会误导用户。

为了使LLM能够在不同领域中得到有效利用，应对这些挑战至关重要。检索增强生成（Retrieval-Augmented Generation，RAG）技术通过引用外部知识，在响应模型查询的同时检索外部数据进行补充，确保了生成内容的准确性和时效性，降低了生成错误内容的概率，提高了LLM在实际应用中的适用性，为应对这些挑战提供了一条有效途径。如图6-1所示，RAG赋予了GPT-3.5在其原始训练数据之外提供精确答案的能力。

![image](https://github.com/user-attachments/assets/145a68e7-f51a-4980-b910-cd4a71d0972a)


#### 6.1 RAG架构分析
RAG是一种提高LLM输出质量的方法，结合了信息检索和生成的优势，通过检索外部数据源，为LLM提供额外的知识信息，从而弥补LLM在训练数据方面的不足。RAG主要用于提升生成任务的性能和效果，尤其是在需要外部知识支持的场景中。


一个典型的RAG框架（见图6-2）分为检索器（Retriever）和生成器（Generator）两部分。检索过程包括对数据（如Documents）做切分、嵌入（Embedding）向量并构建索引（Chunks Vectors），通过向量检索来召回相关结果，而生成过程则是利用基于检索结果（Context）增强的prompt来激活LLM以生成答案（Result），以下是对检索器和生成器的详细介绍。


![image](https://github.com/user-attachments/assets/fc3a50b5-7b9b-420f-8d9e-6ae64581a25c)


#### 6.1.1 检索器
检索器的作用是在大规模的文档集合中快速检索出与输入查询最相关的信息。检索器通常由以下组件构成。
- **文档编码器**：文档编码器负责将文档库中的每个文档转换成固定长度的向量。这通常通过预训练的LLM（如BERT、GPT等）来实现，其能够捕捉文档的语义信息。文档编码器的输出是一个文档向量，它将作为检索过程中的参考点。
- **查询编码器**：查询编码器与文档编码器类似，但它的作用是将用户的输入查询转换成向量。查询编码器同样可以采用预训练的LLM，以确保查询的语义被准确捕捉。 
- **相似度计算**：检索器通过计算查询向量和文档向量之间的相似度来确定相关性。常用的相似度计算方法包括余弦相似度、点积等。根据相似度得分，检索器从文档库中选取最相关的文档或文档片段。 
- **检索策略**：检索策略是RAG模型中检索器的核心组成部分，它决定了如何从大量文档中选取与用户查询最相关的信息。检索策略的设计直接影响了生成器生成响应的质量和相关性。传统的检索方法存在一定的局限性，主要表现在文档块的大小对匹配用户问题的效果有直接影响。具体来说，较大的文档块含有更多内容，当它们被转换成固定维度的向量时，这些向量可能无法精确地表达文档块中的全部内容，导致与用户问题的匹配度降低。相反，较小的文档块虽然内容较少，但转换成向量后能较好地反映其内容，因此匹配度较高。然而，由于信息量有限，这些小文档块可能无法提供全面且准确的答案。为了克服这些挑战，我们可以通过调整和优化检索策略或使用其他检索器，如LangChain中的父文档检索器，该工具有效地解决了文档块大小与用户问题匹配度的问题。

#### 6.1.2 生成器
生成器负责基于检索到的信息和原始查询生成响应。生成器通常是一个基于Transformer架构的解码器，具有以下特点。
- **上下文编码**：生成器在生成过程中不仅考虑用户的原始查询，还会整合检索器提供的相关文档信息。这要求生成器具有强大的上下文编码能力，以便能够理解和利用检索到的信息。
- **自回归生成**：生成器采用自回归的方式逐步生成文本。在每一步的生成过程中，模型都会考虑之前生成的词和检索到的文档信息。这种方式有助于生成连贯且信息丰富的文本，同时可以避免在生成过程中出现重复和冗余的文本。 
- **注意力机制**：生成器内部的注意力机制允许模型在生成每个词时关注检索到的文档的不同部分。注意力权重的动态调整使得生成的文本能够更加准确地反映检索到的信息。

#### 6.2 RAG工作流程
在构建一个RAG系统时，我们通常会遵循以下几个关键步骤。图6-3所示为一个典型的RAG工作流程。

![image](https://github.com/user-attachments/assets/15c0dee5-1fc9-470a-8cb1-dd8c52933dd4)


#### 6.2.1 数据提取
数据提取是获取信息的首要步骤。在这个阶段，系统从预设的数据源中抓取数据。数据源可以有多种，如在线文章库、专业期刊、电子书、新闻存档等。数据源包括多种格式，如Word文档、TXT文件、CSV数据表、Excel表格，甚至是PDF文件、图片和视频等。提取的信息必须是高质量的，因为这将直接影响RAG系统的输出质量。此步骤可能还包括数据清洗，如去除无用的格式信息，或者修正错误，以确保数据的准确性和一致性。

#### 6.2.2 文本分割
文本分割是自然语言处理（NLP）中的基础环节，它用于将文本解构为更易于分析和处理的单元。通过运用先进的NLP技术（如分词、句法分析和实体识别），文本分割可以将文本拆解为单词或短语，还能够识别和保留重要的语言结构和语义信息。例如，分词可以识别复合词和新造词，句法分析能够揭示句子的主语和谓语结构，而实体识别则能够从文本中提取出关键的名词短语和专有名词。在文本分割的过程中，上下文信息发挥着核心作用，它使得分割结果不仅停留在字面意义上，而且能够深入理解每个词汇的深层含义和语句间的逻辑关系。文本分割主要考虑两个因素：embedding（嵌入文本）模型的token限制情况；语义完整性对整体的检索效果的影响。一些常见的文本分割方式如下。
- **句子分割**：以“句子”为粒度进行分割，保留一个句子的完整语义。常见的分割符包括：句号、感叹号、问号、换行符等。
- **固定长度分割**：根据embedding模型的token长度限制，将文本分割为固定长度（如256+512个token），这种分割方式会损失很多语义信息，因此一般通过在文本头尾增加一定的冗余量来缓解。

#### 6.2.3 向量化
向量化是自然语言处理中的一项关键技术，它旨在将复杂的文本数据转换为机器可以理解和处理的数值形式。通过这一过程，原始的高维、非结构化文本数据被编码成低维、结构化向量，从而便于后续计算任务的执行和机器学习模型的应用。

在向量化过程中，文本数据首先被分割成较小的信息单元，如单词、短语或句子。然后，每个信息单元通过特定的算法被映射到一个数值向量上。这些向量不仅捕捉了词汇的语义信息，还尽可能地保留了文本中的上下文关系，这对于理解语言的深层含义至关重要。

一旦文本数据被转换成数值向量形式，这些数值就可以直接被应用于各种算法中，如相似度计算、聚类分析、文本分类和情感分析等。向量化不仅提高了处理效率，还使得机器能够执行复杂的语言任务，从而在自然语言处理领域中实现许多突破性的应用。因此，向量化是连接自然语言与机器智能的桥梁，是实现高级语言处理技术的基础。

向量化过程会直接影响后续检索的效果，目前常见的embedding模型如表6-1所示，如果遇到特殊场景（如涉及一些罕见专有名词或字等）或者想进一步优化效果，则可以选择开源embedding模型进行微调或直接训练适合自己场景的embedding模型。

| 模型名称 | 描述 |
| ---- | ---- |
| ChatGPT-Embedding | 由OpenAI公司提供，以接口形式调用 |
| ERNIE-Embedding V1 | 由百度公司提供，依赖于文心大语言模型能力，以接口形式调用 |
| M3E | 一个功能强大的开源embedding模型，包含m3e-small、m3e-base、m3e-large等多个版本，支持微调和本地部署 |
| BGE | 由北京智谱人工智能研究院发布，同样是一个功能强大的开源embedding模型，包含了支持中文和英文的多个版本，支持微调和本地部署 |

#### 6.2.4 数据检索
在RAG系统的数据检索环节，系统巧妙地利用文本向量化的结果，在广泛的知识库中执行精确的信息检索任务。在这一阶段，系统通过比较文本向量之间的相似度度量（如余弦相似度或欧几里得距离）来识别与用户输入查询最匹配的文档或文档片段。这一过程至关重要，因为它直接影响到系统能否有效地从知识库中抽取出相关性强、信息价值高的内容，从而为生成准确、丰富的答案奠定坚实的基础。通过这种方式，RAG系统不仅能够提供与用户查询紧密相关的信息，还能够在生成过程中考虑到更广泛的上下文和背景知识，极大地提升了生成内容的质量和相关性。常见的数据检索方法包括相似性检索、全文检索等，根据检索效果，也可以结合使用多种检索方法，以提升召回率。
- **相似性检索**：计算查询向量与所有存储向量的相似性得分，返回得分高的记录。常见的相似性计算方法包括余弦相似性、欧式距离、曼哈顿距离等。
- **全文检索**：全文检索是一种比较经典的检索方法，在数据存入时，通过关键词构建倒排索引；在检索时，通过关键词进行全文检索，从而找到对应的记录。

#### 6.2.5 注入提示
在注入提示（prompt）环节，检索得到的信息被巧妙地整合到一个新的文本提示中。这个文本提示是引导RAG系统生成答案的关键输入，它直接影响着系统如何调动和利用检索到的知识。通过精心设计的prompt，RAG系统不仅能够确保生成的内容与用户查询紧密相关，还能够在回答中融入丰富的背景知识和深层次的理解，从而产生更加全面和精准的输出。

prompt的设计只有方法、没有语法，比较依赖于个人经验。prompt一般包括任务描述、背景知识（检索得到）、任务指令（一般是用户提出的问题）等，根据任务场景和大语言模型性能，可以在prompt中适当加入其他指令来优化大语言模型的输出。一个简单的知识问答场景的prompt如下所示。

【任务描述】

假如你是一个专业的客服助理，请根据背景知识回答问题。

【背景知识】{context} // 数据检索得到的相关文本

【问题描述】{question} // 提出的问题


#### 6.2.6 提交给LLM
最后，生成的prompt会被提交给LLM。LLM会处理prompt，并生成最终用户可读的答案。这一阶段利用了先进的机器学习技术，包括深度学习网络，以产生准确、相关且流畅的答案。

整个RAG系统的设计旨在增强传统语言模型的性能，通过结合广泛检索的信息与高级语言生成技术，提供更加丰富和精确的用户体验。在未来，随着技术的进步，我们可以期待这一系统在众多领域中的应用，从智能搜索引擎到个性化教育辅导，再到自动化内容创建，都将极大地受益于RAG技术的发展。

#### 6.3 RAG与微调和提示词工程的比较
在LLM的优化方法中，RAG经常与微调（FT）和提示词工程进行比较，每种方法都有自己独特的特点。提示词工程对模型和外部知识的修改较少，重点是利用LLM自身的能力；微调则需要对模型重新进行训练以实现更新，但可以深度定制模型的行为和风格，还需要使用大量的计算资源进行数据集的准备和训练；RAG有更加灵活的知识获取方式，可以从外部数据源实时检索信息，适用于精确的信息检索任务。在RAG的早期阶段，对模型的修改需求较小，随着研究的进展，模块化RAG会越来越多地与微调技术相结合，在更多的场景中得到更好的应用。图6-4所示为RAG与微调和提示词工程在上下文优化和行为优化能力方面的对比。


![image](https://github.com/user-attachments/assets/b42efe97-1dea-4af6-a2b8-87a9c0fa4396)


#### 6.4 基于LangChain的RAG应用实战
LangChain是开发大语言模型的框架，将多个组件连接在一起，能够轻松管理与大语言模型的交互。下面选用汝瓷百科文本作为外部数据，基于LangChain开发框架和ERNIE-Bot大语言模型，构建一个RAG外部知识问答应用。

#### 6.4.1 基础环境准备
首先，安装环境需要依赖的Python包，包括用于编排的langchain、向量数据库chromadb、大语言模型接口langchain_wenxin。
```
pip install langchain chromadb langchain_wenxin
```
然后，在千帆大模型平台上创建ERNIE大语言模型应用，申请大语言模型的API Key、Secret Key，申请界面如图6-5所示。


![image](https://github.com/user-attachments/assets/5f78cbd6-8ed9-4565-8e6d-53505398875f)


#### 6.4.2 收集和加载数据
首先，收集所需的外部数据。这里以Python爬取汝瓷百科文本为例，获取并保存外部数据，示例代码如下：
```python
import requests
from bs4 import BeautifulSoup
url = "https://baike.baidu.com/item/汝瓷"
response = requests.get(url)
response.encoding = response.apparent_encoding
soup = BeautifulSoup(response.text, 'html.parser')
title = soup.find('h1').get_text()
# class_='你需要查询的类名'
summary = ''.join([x.get_text() for x in soup.find('div', class_='lemmaSummary-E1R06 J-summary').find_all('span',class_='text_sf17Lt')])
concent = '\n\n'.join([title,summary])
for para in soup.find_all('div', class_='J-lemma-content')[0]:
    if para.find('h2'):
        concent = '\n\n'.join([concent] + [para.find('h2')['name']+ '\n' + para.find('h2').get_text()+'\n'])
    if para.find('span', class_='text_sf17Lt'):
        concent = ''.join([concent] + [x.get_text() for x in para.find_all('span', class_='text_sf17Lt')])
concent = concent.replace('  ', '')
with open('汝瓷.txt', 'w') as f:
    f.write(concent)
```
然后，加载收集到的数据。LangChain中有多个内置的DocumentLoaders，这里使用TextLoader，用于加载上一步爬虫保存的文本数据。代码如下所示，生成的documents是一个只含有原始状态下Document的列表，在Document中含有文本内容和元数据信息。
```python
from langchain.document_loaders import TextLoader
loader = TextLoader('./汝瓷.txt')
document = loader.load()
```

#### 6.4.3 分割原始文档
原始Document中的文本内容过长，无法适应大语言模型的上下文窗口，需要将其分割成更小的Document。LangChain内置了许多文本分割器，这里使用CharacterTextSplitter，设置chunk_size为100、chunk_overlap为10，以保持块之间的文本连续性，代码如下：
```python
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)
documents = text_splitter.split_documents(documents)
```

#### 6.4.4 数据向量化后入库
对分割后的文档数据进行向量化处理，并写入向量数据库。这里选用CoRom通用中文嵌入文本模型作为embedding模型，选用Chroma作为向量数据库，代码如下：
```python
from langchain.embeddings import ModelScopeEmbeddings
from langchain.vectorstores import Chroma
model_id = "damo/nlp_corom_sentence-embedding_chinese-base"
embeddings = ModelScopeEmbeddings(model_id=model_id)
db = Chroma.from_documents(documents, embedding=embeddings)
```

#### 6.4.5 定义数据检索器
向量数据库被填充后，可以将其定义为检索器组件。该组件能够根据用户查询和嵌入块之间的语义相似性进行相似性检索，从而找到最相关的内容。
```python
retriever = db.as_retriever()
```

#### 6.4.6 创建提示
prompt作为大语言模型的直接输入，能帮助大语言模型生成更符合要求的输出结果。在LangChain中可以使用ChatPromptTemplate来创建一个提示模板，告诉大语言模型如何使用检索到的上下文来回答问题。下面是本次设计和创建的prompt。
```python
from langchain.prompts import ChatPromptTemplate
template = """
【任务描述】
你是善于总结归纳的文本助理，请根据背景知识和聊天记录回答问题，并遵守回答要求。
【回答要求】
- 你需要严格根据背景知识提供最相关的答案，不要通过拓展知识范围来回答问题。
- 对于不知道的信息，直接回答“未找到相关答案”。
- 答案不仅要简洁清晰，还要全面。
【背景知识】{context}
【问题描述】{question}
【聊天记录】{chat_history}
"""
prompt = ChatPromptTemplate.from_template(template=template)
```

####


#### 6.4.7 调用LLM生成答案
创建ConversationalRetrievalChain（对话检索链），并传入大语言模型、检索器和记忆系统。ConversationalRetrievalChain首先会将聊天记录和新提问的问题整合成一个新的独立问题，然后利用检索器从向量数据库中检索相关的文档片段，再将这些文档片段和问题一起作为提示发送给大语言模型，大语言模型根据这些信息生成回答。示例代码如下：
```python
from langchain.chat_models import ChatAnthropic
from langchain.chains import ConversationalRetrievalChain
llm = ChatAnthropic(model="claude-2", anthropic_api_key="your_api_key") 
# 这里使用Anthropic的Claude-2模型示例，实际应用中请替换为合适的大语言模型及API密钥
chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)
chat_history = []
while True:
    question = input("请输入问题（输入exit退出）：")
    if question.lower() == "exit":
        break
    result = chain({"question": question, "chat_history": chat_history})
    print(result["answer"])
    chat_history.append((question, result["answer"]))
```

通过以上步骤，基于LangChain和ERNIE - Bot构建的RAG外部知识问答应用能够根据用户输入的问题，从汝瓷百科文本中检索相关信息，并利用大语言模型生成准确的回答，实现了外部知识的有效整合与应用。 



### 第6章 RAG检索架构分析与应用
问题，以传递给向量数据库并查找相关文档，然后将获取的知识和生成的新问题注入prompt，让大语言模型生成答案。

```python
from langchain import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_wenxin.llms import Wenxin

# LLM
llm = Wenxin(model="ernie-bot", baidu_api_key="api_key", baidu_secret_key="secret_key")
retriever = db.as_retriever()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
retriever = db.as_retriever()
conversation_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    memory=conversation_memory,
    combine_docs_chain_kwargs={"prompt": prompt}
)

question = "汝瓷有哪些产品特点?"
response = qa_chain({"question": question})
output = response['answer']
print(output)
```

这样就完成了一个定制化RAG外部知识问答应用的构建，输出结果如图6-6所示。回答内容基本上与文档信息保持一致，没有出现“幻觉”现象。


![image](https://github.com/user-attachments/assets/29cc5ef2-ca28-41c2-917b-a8cc4085c1bd)


汝瓷的产品特点主要包括以下几个方面：

1. 以烧制青瓷为主，釉色有天青、天蓝、豆绿、月白等多种，其中天青釉最为著名。汝瓷的天

2. 青釉瓷中，釉中含有玛瑙，色泽独特，有“雨过天晴云破处”“千峰碧波翠色来”之美妙。观其釉色

3. ，犹如“玉”，扣声如磬，质感甚佳，有“似玉非玉而胜似玉”之说。


4. 汝瓷的釉色温润自然，有“雨过天晴云破处”之誉。釉面隐现出一种柔和含蓄的光泽，它既不同于钧瓷的乳光，也不同于官窑的脂光，而是具有一种类似古玉般内蕴的光泽。


5. 汝瓷的胎色多为香灰色或浅灰色，胎质坚硬细密，胎壁较薄，器形规整。汝瓷的胎釉结合紧密，釉层较厚，不易脱落。


6. 汝瓷的装饰手法多样，常见的有刻花、划花、印花等。汝瓷的装饰题材以花卉、动物、人物
   等为主，构图简洁大方，线条流畅自然。

总的来说，汝瓷以其独特的釉色、胎质和装饰手法等特点，成为中国陶瓷史上的珍品之一。

本章主要介绍了RAG技术的架构，并实践了如何利用Python和LangChain实现RAG流程的构建，该流程使用了ERNIE - Bot大语言模型、Chroma向量数据库和CoRom通用中文嵌入文本模型，并通过LangChain进行编排。通过学习本章可以看到，RAG为解决大语言模型在处理特定、最新和专有信息等方面的不足，提供了一个有效且灵活的方法。RAG为大语言模型提供了额外的、来自外部知识源的信息，使其在生成更为精确、更加贴合上下文答案的同时，能有效减少误导性信息的产生。通过6.4节的实战练习，读者也可以清晰地了解到RAG的工作流程和实际应用。 
